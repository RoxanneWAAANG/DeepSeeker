DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding Zhiyu Wuâˆ—, Xiaokang Chenâˆ—, Zizheng Panâˆ—, Xingchao Liuâˆ—, Wen Liuâˆ—,â€ , Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, Zhenda Xie, Yu Wu, Kai Hu, Jiawei Wang, Yaofeng Sun, Yukun Li, Yishi Piao, Kang Guan, Aixin Liu, Xin Xie, Yuxiang You, Kai Dong, Xingkai Yu, Haowei Zhang, Liang Zhao, Yisong Wang, Chong Ruanâ€¡ DeepSeek-AI Abstract We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE) Vision- Language Models that significantly improves upon its predecessor, DeepSeek-VL, through two key major upgrades. For the vision component, we incorporate a dynamic tiling vision encoding strategy designed for processing high-resolution images with different aspect ratios. For the language component, we leverage DeepSeekMoE models with the Multi-head Latent Attention mechanism, which compresses Key-Value cache into latent vectors, to enable efficient inference and high throughput. Trained on an improved vision-language dataset, DeepSeek-VL2 demon- strates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding. Our model series is composed of three variants: DeepSeek-VL2-Tiny, DeepSeek-VL2- Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated parameters respectively. DeepSeek- VL2 achieves competitive or state-of-the-art performance with similar or fewer activated param- eters compared to existing open-source dense and MoE-based models. Codes and pre-trained models are publicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2 . 0 2 4 6 8 10 Activated Parameters (Billions)4854606672Average PerformanceDeepSeek-VL2-TinyDeepSeek-VL2-SmallDeepSeek-VL2 InternVL2-1BInternVL2-2BInternVL2-4BInternVL2-8B Qwen2-VL-2BQwen2-VL-7B Phi-3.5-Vision DeepSeek-VL2 Family InternVL2 Family Qwen2-VL Family Figure 1|Average performance vs. activated parameters among different open-source models . We average the accuracy of MMBench v1.1, MMStar, MMMU (Val), MathVista (TestMini), AI2D (Test), and OCRBench. The scores of OCRBench are divided by 10 to scale them to [0, 100]. âˆ—: Core contributors.â€ : Project lead.â€¡: Corresponding author.arXiv:2412.10302v1 [cs.CV] 13 Dec 2024 Contents 1 Introduction 3 2 Model Architecture 4 3 Data Construction 6 3.1 Vision-Language Alignment Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 3.2 Vision-Language Pretraining Data . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 3.3 Supervised Fine-tuning Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 4 Training Methodology 9 4.1 Training Pipelines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 4.2 Hyperparameters and Infrastructures . . . . . . . . . . . . . . . . . . . . . . . . . 10 5 Evaluation 11 5.1 Multimodal Performance . . . . . . . . . . . . . . .

Hyperparameters and Infrastructures . . . . . . . . . . . . . . . . . . . . . . . . . 10 5 Evaluation 11 5.1 Multimodal Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 5.2 Qualitative Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 6 Conclusion 20 2 1. Introduction Large Vision-Language Models (VLMs) have emerged as a transformative force in artificial intelligence [ 15,54,59,63,83,88,94], extending the remarkable capabilities of Large Language Models (LLMs) to seamlessly process both visual and textual information. This advancement has dramatically expanded the potential for AI systems to tackle complex real-world applications that require multimodal understanding. In this technical report, we present DeepSeek-VL2, a new series of open-source Vision- Language Models that leverages the Mixture-of-Experts (MoE) architecture to achieve substantial improvements in both performance and efficiency compared to its predecessor, DeepSeek- VL [ 59]. Our advancements center around three key aspects: (1) a dynamic, high-resolution vision encoding strategy that enhances visual understanding, (2) an optimized language model architecture that significantly improves both training and inference efficiency, and (3) a refined vision-language data construction pipeline that not only boosts overall performance but also extends model capabilities to new areas such as precise visual grounding. For the vision component, we introduce a dynamic tiling vision encoding strategy that efficiently processes high-resolution images of varying aspect ratios. This approach improves over DeepSeek-VLâ€™s hybrid vision encoder, which extracted features from images at two fixed resolutions ( 384Ã—384and 1024Ã—1024 ). Our approach avoids the limitations of the old fixed- size encoder and excels in tasks requiring ultra-high resolution, including visual grounding, document/table/chart analysis, and detailed feature extraction, while maintaining a manageable number of visual tokens. Drawing inspiration from established slicing-tile methods, our system dynamically segments high-resolution inputs into local tiles, processes each tile through a shared vision transformer, and seamlessly integrates the extracted features within the language model. This design preserves the advantages of vision transformers with local attention, enabling rich feature extraction without the quadratic computational scaling typically associated with increasing image resolutions. For the language component, we leverage DeepSeek language models [ 20,53], featuring the Multi-head Latent Attention (MLA) mechanism. MLA significantly reduces computational cost by compressing the Key-Value (KV) cache into a latent vector, resulting in faster inference and increased throughput capacity. We further enhance efficiency through the DeepSeekMoE frame- work [ 20,86], which employs sparse computation techniques. Our model series adopt three MoE variants, 3B, 16B, and 27B. These LLMs have 0.57B, 2.4B, and 4.1B activated parameters respectively. We also greatly enhance our vision-language training data in terms of quality, quantity, and diversity. This comprehensive dataset enables better generalization and

[ 20,86], which employs sparse computation techniques. Our model series adopt three MoE variants, 3B, 16B, and 27B. These LLMs have 0.57B, 2.4B, and 4.1B activated parameters respectively. We also greatly enhance our vision-language training data in terms of quality, quantity, and diversity. This comprehensive dataset enables better generalization and performance across a broad spectrum of tasks, including Visual Question Answering (VQA), Optical Character Recognition (OCR), document/table/chart understanding, visual reasoning, and general chatbot applications. The improved training data has also enabled new abilities such as visual grounding and Graphical User Interface (GUI) perception. In summary, DeepSeek-VL2 marks a substantial leap forward in large-scale Mixture-of- Experts Vision-Language modeling. Through a new visual processing strategy and an optimized language model, we develop a series of models that balances performance with efficiency. By open-sourcing the pre-trained models, we aim to accelerate progress in the field and promote collaborative research advancement. 3 VisionEncoderDeepSeek-MoE â€¦Describethisimageindetail.Thisimagedisplaysâ€¦ VL AdaptorImagetokens DynamicTilingFigure 2|Overview of DeepSeek-VL2 . The overall structure is a llava-style architecture, which includes a vision encoder, a VL adaptor, and a MoE-based LLM. 2. Model Architecture DeepSeek-VL2 consists of three core modules: (1) a vision encoder, (2) a vision-language adaptor, and (3) a Mixture-of-Experts language model. Building upon the decoder-only LLaVA- style [ 54] architecture of its predecessor, DeepSeek-VL2 introduces two major advancements: a dynamic tiling strategy and a DeepSeekMOE [ 20,86] language model featuring Multi-head Latent Attention [ 53]. These innovations enable more efficient processing of both high-resolution visual inputs and text data. Dynamic Tiling Strategy. The original DeepSeek-VL employed a hybrid vision encoder combining SigLIP [ 106] for coarse-grained feature extraction at 384Ã—384resolution and SAM- B [35] for fine-grained feature extraction at 1024Ã—1024 resolution. While this fusion approach generated rich visual representations suitable for various vision-language tasks, it was limited by the fixed 1024Ã—1024 resolution constraint. This limitation is particularly challenging for processing images with larger resolutions and extreme aspect ratios, such as those found in InfographicVQA [67], dense OCR, and detailed visual grounding tasks. Inspired by recent advances in VLMs [ 16,21,55], we implement a dynamic tiling strategy by splitting a high-resolution image into tiles. This approach enables the efficient processing of different high-resolution images with varying aspect ratios using a single SigLIP-SO400M-384 vision encoder [ 106]. The pre-trained SigLIP operates at a base resolution of 384Ã—384. To accommodate different aspect ratios, we define a set of candidate resolutions: ğ¶ğ‘…={(ğ‘šÂ·384,ğ‘›Â· 384) |ğ‘šâˆˆN,ğ‘›âˆˆN, 1â‰¤ğ‘š,ğ‘›,ğ‘šğ‘›â‰¤9}, whereğ‘š:ğ‘›represents the aspect ratio. For an input image of size(ğ»,ğ‘Š), we calculate the padding area required for resizing1it to each candidate resolution in ğ¶ğ‘…. We select the resolution (ğ‘šğ‘–Â·384,ğ‘›ğ‘–Â·384)that minimizes the padding area. The resized image is then divided into ğ‘šğ‘–Ã—ğ‘›ğ‘–local tiles of 384Ã—384pixels, plus one global thumbnail tile. The SigLIP-SO400M-384 vision encoder processes all (1+ğ‘šğ‘–Ã—ğ‘›ğ‘–)tiles, yielding 27Ã—27=729visual embeddings of 1152 dimensions per tile. For computational efficiency and context length management, we disable the dynamic tiling strategy when processing multiple (>2)images. 1We first resize the original image until its long side matches the target resolution, then pad the other dimension while

vision encoder processes all (1+ğ‘šğ‘–Ã—ğ‘›ğ‘–)tiles, yielding 27Ã—27=729visual embeddings of 1152 dimensions per tile. For computational efficiency and context length management, we disable the dynamic tiling strategy when processing multiple (>2)images. 1We first resize the original image until its long side matches the target resolution, then pad the other dimension while maintaining the original aspect ratio. 4 \n\n \n\nFlatten \n\n\n\n \n\n\n\nViewSeparator TokensepInput imageLocalTilesNew Line TokenEncode&MergeDynamic TilingEncode&MergeFlattenFigure 3|Illustration of dynamic tiling strategy in DeepSeek-VL2 . By dividing images into multiple tiles, DeepSeek-VL2 achieves stronger fine-grained understanding capabilities compared to DeepSeek-VL. Table 1|Architectural configuration for DeepSeek-VL2 . We list the hyperparameters of the architecture along with the details related to the mixture-of-expert training. DeepSeek-VL2-Tiny DeepSeek-VL2-Small DeepSeek-VL2 Vocabulary size 129,280 102,400 129,280 Embedding size 1,280 2,048 2,560 #Attention heads 10 16 32 #Layers 12 27 30 Attention Multi-Head Attention MLA (rank=512) MLA (rank=512) #Routed experts 64 64 72 #Shared experts 2 2 2 Top-K for expert selection 6 6 6 Routing function Softmax Softmax Sigmoid Expert correction bias Ã— Ã— âœ“ Vision-Language Adaptor. Following visual tile processing, we implement a 2Ã—2pixel shuffle operation to compress each tileâ€™s visual tokens from 27Ã—27to14Ã—14=196tokens. We then introduce three special tokens when processing the (1+ğ‘šğ‘–Ã—ğ‘›ğ‘–)tiles. For the global thumbnail tile ( 14Ã—14), we add 14 <tile_newline> tokens to the end of each row, resulting in a total number of 14Ã—15=210tokens. For the ğ‘šğ‘–Ã—ğ‘›ğ‘–local tiles, which are arranged in a 2D grid of shape(ğ‘šğ‘–Â·14,ğ‘›ğ‘–Â·14), we append ğ‘šğ‘–Â·14<tile_newline> tokens at the end of the final column to indicate the end of a row of all the local tiles. Additionally, a <view_separator> token is inserted between the global thumbnail tile and the local tiles. The complete visual sequence contains 210+1+ğ‘šğ‘–Â·14Ã—(ğ‘›ğ‘–Â·14+1)visual tokens, which are subsequently projected into the language modelâ€™s embedding space using a two-layer multilayer perceptron (MLP). A visual illustration of our dynamic tiling strategy is shown in Figure 3. DeepSeekMoE LLM. Our language model is based on DeepSeekMoE [ 20,86], which incor- porates the Multi-head Latent Attention mechanism [ 53]. MLA enhances inference efficiency by compressing the Key-Value cache into a latent vector, enabling increased throughput capacity. The model also incorporates a MoE architecture [ 20] allowing for efficient inference through sparse computation. During MoE training, we introduce a global bias term [ 86] for each expert to cost-effectively improve load balancing between experts. DeepSeek-VL2 comes in three variants with the following model sizes: 1.0B, 2.8B and 4.5B. Complete architectural specifications can be found in Table 1. 5 3. Data Construction We build a comprehensive Vision-Language dataset from diverse sources for DeepSeek-VL2. The training process is structured into three distinct stages: (1) VL alignment, (2) VL pretraining, and (3) supervised fine-tuning (SFT). In the following parts, we provide descriptions of the data used in each stage. 3.1. Vision-Language Alignment Data The alignment stage focuses on training the MLP connector to bridge the pretrained visual encoder and the LLM. For this initial warmup phase, we utilize ShareGPT4V [ 12], a dataset containing approximately 1.2M caption and conversation samples. 3.2. Vision-Language Pretraining

the data used in each stage. 3.1. Vision-Language Alignment Data The alignment stage focuses on training the MLP connector to bridge the pretrained visual encoder and the LLM. For this initial warmup phase, we utilize ShareGPT4V [ 12], a dataset containing approximately 1.2M caption and conversation samples. 3.2. Vision-Language Pretraining Data Following DeepSeek-VL [ 59], our pretraining data combines vision-language (VL) and text-only data to maintain a balance between VL capabilities and text-only performance. For DeepSeek- VL2, we maintain a ratio of around 70% VL data to 30% text-only data, with the latter sourced directly from our base LLM pretraining corpus. In the following, we categorize the VL data into several groups and describe their details. Interleaved image-text data. Our data collection begins with several open-sourced datasets, including WIT [ 79], WikiHow [ 38], and 30% random samples from OBELICS [ 41]. This specific mixing ratio was determined through preliminary experiments with DeepSeek-VL2-Tiny. To enhance multilingual capabilities, we supplemented the predominantly English datasets with Chinese content extracted from Wanjuan [ 29]. Additionally, we developed an in-house collection to expand coverage of general real-world knowledge. Image captioning data. Image captions represent fundamental data in VLM training, provid- ing direct alignment between visual and textual information. We initially leveraged diverse open-source datasets [ 8,25,28,36,37,39,40,48,50,51,73,78,80,82]. However, our preliminary analysis revealed severe quality variations across these datasets, ranging from dense, accurate captions generated by advanced VLMs to problematic cases with brief descriptions, mismatched text pairs, or obvious hallucinations. To address these quality inconsistencies, we developed a comprehensive image captioning pipeline that considers: (1) OCR hints, (2) meta information (e.g., location, camera settings), and (3) relevant original captions as prompts. Using an in-house captioner, we recaption the images following prompting strategies similar to PixelProse [ 78], employing varied instructions to guide the VLMâ€™s caption generation. Despite the overall improvement in caption quality, we observed repetition issues in the large-scale annotation pipelines. To mitigate this, we implemented a quality control pipeline using DeepSeek Chat [ 53] to score all captions simply based on their writing quality. In practice, this approach is both efficient and effective in filtering out low-quality captions. Optical character recognition data. To develop OCR capabilities, we used open-source datasets including LaTeX OCR [ 7] and 12M RenderedText [ 93]. We combined these datasets with an extensive in-house OCR dataset covering diverse document types. Currently, our in-house 6 dataset mainly focuses on English and Chinese character recognition. We plan to expand to other languages in our future work. Visual question-answering (QA) data. In our early exploration, we found general QA data clearly benefits model pretraining. Consequently, we developed a comprehensive visual QA dataset consisting of the following categories: â€¢General VQA. We inherit the general VQA data from DeepSeek-VL. For more details, please refer to [59]. â€¢Table, chart and document understanding. We adopt PubTabNet [ 112], FinTabNet [ 111] and Docmatix [42] to enhance document comprehension capabilities. â€¢Web-to-code and plot-to-Python generation. We leverage Websight [ 44] for webpage- to-code abilities and Python plots obtained from

VQA data from DeepSeek-VL. For more details, please refer to [59]. â€¢Table, chart and document understanding. We adopt PubTabNet [ 112], FinTabNet [ 111] and Docmatix [42] to enhance document comprehension capabilities. â€¢Web-to-code and plot-to-Python generation. We leverage Websight [ 44] for webpage- to-code abilities and Python plots obtained from public Jupyter notebooks, following DeepSeek-VL. We enhance this dataset by replicating a portion of Websight using DeepSeek V2.5. We also exploit Python plot codes generated by DeepSeek V2.5 to mitigate the noises in the plot-to-code data. â€¢QA with visual prompt. We follow [ 9] to construct visual prompt understanding data by overlaying various visual indicators (arrows, boxes, circles, and scribbles) onto images from [ 9,89,90]. We then created QA pairs focusing on objects highlighted by these visual prompts. Visual grounding data. We construct our visual grounding dataset from [ 71,75]. For each imageâ€™s object detection annotations, we structure the data as follows: â€¢ Prompt: Locate <|ref|><query><|/ref|> in the given image. â€¢ Response: <|ref|><query><|/ref|><|det|>[[x1, y1, x2, y2],...]<|/det|> during training, the question prompts are randomly sampled from a candidate pool during training. <|ref|> ,<|/ref|> ,<|det|> ,<|/det|> are special tokens. <query> is a place- holder for either the category name (e.g., â€œcarâ€) or description of the object (e.g., â€œthe leftmost personâ€). [[x1, y1, x2, y2], ...] is a list of bounding boxes, where each bounding box corresponds to an objectâ€™s position. The coordinates x1, y1 andx2, y2 specify the top-left and bottom-right corners respectively, normalized to values between 0 and 999 according to the resolution of the image. We also construct negative samples where queried objects are intentionally absent from the images to enhance the robustness of the model. Grounded conversation data. We derived our grounded conversation dataset from [ 71], struc- tured in the following format: â€¢ Prompt: <|grounding|>Can you describe the content of the image? â€¢Response: Two <|ref|>dogs<|/ref|><|det|>[[x1, y1, x2, y2],...]<|/det|> are running on the grass. As in other visual grounding data, <|grounding|> ,<|ref|> ,<|/ref|> ,<|det|> ,<|/det|> are special tokens and x1, y1, x2, y2 is subject to the same normalization scheme. 7 3.3. Supervised Fine-tuning Data Our SFT data combines a diverse collection of open-sourced datasets with high-quality in-house QA pairs. Below, we detail our efforts to enhance the quality of our SFT dataset. General visual question-answering. While public visual QA datasets are diverse [ 9,10,27, 31,43,47,74], they often suffer from three main limitations: (1) short responses, (2) poor OCR quality, and (3) hallucinated content. To address these issues, we regenerate responses by jointly considering the original questions, images, and OCR information. Our experiments demonstrate that this approach produces more comprehensive and accurate results. During development, we observed that an early version of DeepSeek-VL2, particularly the Tiny variant, occasionally inserted English words inappropriately in Chinese responses. This issue was not present in our larger models, suggesting it stemmed from limited model capacity and an imbalance between English and Chinese data in the visual-language pretraining stage. To address this limitation in our smaller model, we developed an in-house Chinese QA dataset

English words inappropriately in Chinese responses. This issue was not present in our larger models, suggesting it stemmed from limited model capacity and an imbalance between English and Chinese data in the visual-language pretraining stage. To address this limitation in our smaller model, we developed an in-house Chinese QA dataset with diverse image descriptions and single/multi-round conversations. This dataset helps to mitigate the language mixing issue. Furthermore, we created an extra in-house dataset to complement real-world and cultural visual knowledge, including anime, memes, cuisine and art. OCR and document understanding. Thanks to our advanced image captioning pipeline, DeepSeek-VL2 already demonstrates superior OCR capabilities compared to other state-of-the- art VLMs. Therefore, rather than further enhancing OCR performance during the SFT stage, we focused on cleaning existing open-source datasets [ 24,31,43,66,67,77,92,104] by removing samples with poor OCR quality. For document understanding, we curated a diverse subset of document pages from our in-house data. We then generate multi-round conversational QA pairs specific to document comprehension. Early results indicate that this approach improves document-based interactions. Table and chart understanding. We enhanced table-based QA data by regenerating responses for all public datasets [ 14,49] based on their original questions except Cauldron [ 43], which already exhibits high quality. Similar to our OCR capabilities developed during VL pretrain- ing, our model demonstrated strong performance in chart understanding without requiring additional efforts. Reasoning, logic, and mathematics. We enhance public reasoning-focused datasets [ 17,43,61, 76,102,109] with more detailed reasoning processes and standardize response formats which puts the final answer at the end of the response. We observe that detailed responses are less effective when training smaller VLMs. In our exploration, DeepSeek-VL2-Tiny shows better performance with more concise responses. Textbook and academic questions. We build an internal dataset focused on textbooks from our document collection. This dataset primarily emphasizes college-level contents across multiple academic disciplines. Web-to-code and plot-to-Python generation. We expand our in-house dataset for web code and Python plot code beyond what was used during pretraining. For open-source datasets, we 8 improve their quality by regenerating their answers. Visual grounding. We develop our visual grounding dataset using data from [ 2,23,64,85, 101,110]. To boost model capabilities, we translate query phrases into Chinese and create additional negative samples. We also add in-context visual grounding data, where the task involves locating objects of the same category across multiple images, given a reference object highlighted by a rectangle or ellipse in a reference image. The data format follows this structure: â€¢Prompt: <|grounding|>The first image shows <object>.Please identify the object of the same category in the second image. â€¢Response: <|ref|><description><|/ref|><|det|>[[x1, y1, x2, y2]]<|/det|> In this format, <|grounding|> ,<|ref|> ,<|/ref|> ,<|det|> ,<|/det|> are special to- kens. The <object> placeholder represents phrases like â€œan object within the red bounding boxâ€ while <description> is the modelâ€™s description of the detected object (e.g., â€œcatâ€). Grounded conversation. We construct our grounded conversation data using [ 62,72] to further enhance the modelâ€™s capabilities established during the pretraining phase. Text-Only datasets. To maintain the language ability of the model,

object within the red bounding boxâ€ while <description> is the modelâ€™s description of the detected object (e.g., â€œcatâ€). Grounded conversation. We construct our grounded conversation data using [ 62,72] to further enhance the modelâ€™s capabilities established during the pretraining phase. Text-Only datasets. To maintain the language ability of the model, we also use text-only instruction-tuning datasets [4, 6, 18, 19, 68, 70, 84, 91, 98] during the SFT stage. 4. Training Methodology 4.1. Training Pipelines DeepSeek-VL2 is trained through a three-stage pipeline: (1) an initial stage where we train the vision encoder and vision-language adaptor MLP while keeping the language model fixed, using image-text paired data detailed in Section 3.1, (2) a pretraining stage where we conduct vision- language pre-training using the data described in Section 3.2, and (3) a fine-tuning stage where we perform supervised fine-tuning with the data outlined in Section 3.3. In both the pretraining and fine-tuning stages, all model parameters, including the vision encoder, vision-language adaptor, and language model, are unlocked and trained simultaneously. Throughout all stages, we emphasize visual understanding capabilities and compute the next token prediction loss exclusively on the text tokens. Vision-Language Alignment. Building upon pre-trained language models (DeepSeekMoE 3B/16B/27B), our primary objective is to establish robust connections between visual features and language features. This alignment enables the pre-trained language model to effectively handle visual inputs. Unlike previous approaches [ 54,59], which maintain fixed pretrained vision encoders and language models, we adapt the fixed-resolution vision encoder to accom- modate dynamic high-resolution images. In this stage, we optimize both the vision encoder and vision-language adaptor while keeping the language model frozen. Vision-Language Pre-training. After establishing the vision-language alignment in the embedding space, we dedicate the majority of our computational resources to vision-language pre-training. This stage focuses on developing comprehensive joint vision-language knowledge across diverse tasks. We unfreeze all parameters, including the vision encoder, vision-language 9 Table 2|Hyperparameters for training DeepSeek-VL2 . The Step LR Scheduler divides the learning rate byâˆš 10 at 50% and 75% of the total training steps. DeepSeek-VL2-Tiny DeepSeek-VL2-Small DeepSeek-VL2 Total parameters (LLM) 3B 16B 27B Activated parameters (LLM) 0.57B 2.4B 4.1B Vision Encoder SigLIP-SO400M SigLIP-SO400M SigLIP-SO400M Hyperparameters Stage 1 Stage 2 Stage 3 Stage 1 Stage 2 Stage 3 Stage 1 Stage 2 Stage 3 Learning rate 5.4Ã—10âˆ’45.4Ã—10âˆ’43.0Ã—10âˆ’54.2Ã—10âˆ’44.2Ã—10âˆ’41.4Ã—10âˆ’54.5Ã—10âˆ’44.5Ã—10âˆ’42Ã—10âˆ’5 Visual Encoder LR multiplier 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 Fix langauge model âœ“Ã— Ã— âœ“Ã— Ã— âœ“Ã— Ã— LR scheduler Cosine Step Constant Cosine Step Constant Cosine Step Constant Weight decay 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 Gradient clip 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 Optimizer AdamW(ğ›½1=0.9,ğ›½2=0.95) AdamW(ğ›½1=0.9,ğ›½2=0.95) AdamW(ğ›½1=0.9,ğ›½2=0.95) BF16 optimizer Ã— Ã— Ã— Ã— Ã— Ã— âœ“ âœ“ âœ“ Aux loss weight 0.001 0.001 0.001 0.001 0.001 0.001 0.0001 0.0001 0.0001 Expert bias correction step - - - - - - 0 0.001 0 Training tokens 2.0B 798.5B 19.5B 2.0B 808.9B 20.0B 2.0B 796.5B 19.5B Batch size 256 2304 64 256 2304 64 256 3360 64 Sequence length 4096 4096 4096

weight 0.001 0.001 0.001 0.001 0.001 0.001 0.0001 0.0001 0.0001 Expert bias correction step - - - - - - 0 0.001 0 Training tokens 2.0B 798.5B 19.5B 2.0B 808.9B 20.0B 2.0B 796.5B 19.5B Batch size 256 2304 64 256 2304 64 256 3360 64 Sequence length 4096 4096 4096 4096 4096 4096 4096 4096 4096 Sequence packing Ã— âœ“ âœ“ Ã— âœ“ âœ“ Ã— âœ“ âœ“ Pipeline parallelism Ã— âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ adaptor MLP , and DeepSeekMoE LLM, to enable full model optimization. Using approximately 800B image-text tokens (Section 3.2), this stage significantly enhances the modelâ€™s multimodal understanding capabilities while maintaining most of its language capabilities. Supervised Fine-T uning. In the final stage, we enhance the pre-trained modelâ€™s instruction- following and conversational capabilities through supervised fine-tuning. Using our in-house vision-language SFT data, we optimize all parameters while supervising only the answers and special tokens, masking both system and user prompts. To strengthen dialogue comprehension, we combine multimodal data with the pure text dialogue data from DeepSeek-V2 [ 53]. This approach ensures robust performance across diverse vision-language tasks, including dense image captioning, general VQA, OCR, table/chart/document/figure understanding, visual-to- code, visual reasoning, visual grounding, and language understanding, etc.. 4.2. Hyperparameters and Infrastructures Detailed hyperparameters for DeepSeek-VL2 training are listed in Table 2. We conducted our training and evaluation using HAI-LLM [ 30], an efficient and lightweight platform designed for large models. A significant challenge in our pipeline parallel strategy arose from the vision encoderâ€™s unique computational characteristics compared to LLM blocks. As the first component in the model pipeline, the vision encoder requires careful load balancing across GPUs to prevent pipeline bubbles and optimize GPU utilization. To address this, we implemented fine-grained layer division of the vision encoder within our pipeline parallel strategy. Moreover, we perform image tile load balancing across different data parallel ranks during the forward and backward processes to alleviate the imbalance in the number of image tiles caused by the dynamic resolution strategy. Our training process also incorporates tensor parallelism and expert parallelism approaches to achieve the highest efficiency. Since some data batches have only text data while others include image data, we introduce two different pipeline strategies 10 Table 3|Comparison with state-of-the-art models on OCR-related multimodal benchmarks .â€ : activated parameters of MoE model. Model #Params (LLM) #Params (VE) #Params (Activated)DocVQA ChartQA InfoVQA TextVQA OCRBench (test) (test) (test) (val) Closed Model GPT-4V [69] - - - 87.2 78.1 75.1 78.0 645 GPT-4o [32] - - - 92.8 85.7 79.2 77.4 736 Claude 3.5 Sonnet [5] - - - 95.2 90.8 74.1 74.1 788 Gemini-1.5-Pro [81] - - - 93.1 87.2 80.1 78.7 754 Open-source Model (0.5B - 3B) LLaVA-OV 0.5B [45] 0.5B 0.4B 0.9B 70.0 61.4 41.8 - - InternVL2-1B [16] - - 0.9B 81.7 72.9 50.9 70.5 754 MM 1.5-1B [107] - - 1B 81.0 67.2 50.5 72.5 605 DeepSeek-VL2-Tiny 0.6Bâ€ 0.4B 1.0Bâ€ 88.9 81.0 66.1 80.7 809 MolmoE-1B [22] 1.2Bâ€ 0.3B 1.5Bâ€ 77.7 78.0 53.9 78.8 MiniCPM-V 2.0 [99] 2.4B 0.4B 2.8B 71.9 -

0.4B 0.9B 70.0 61.4 41.8 - - InternVL2-1B [16] - - 0.9B 81.7 72.9 50.9 70.5 754 MM 1.5-1B [107] - - 1B 81.0 67.2 50.5 72.5 605 DeepSeek-VL2-Tiny 0.6Bâ€ 0.4B 1.0Bâ€ 88.9 81.0 66.1 80.7 809 MolmoE-1B [22] 1.2Bâ€ 0.3B 1.5Bâ€ 77.7 78.0 53.9 78.8 MiniCPM-V 2.0 [99] 2.4B 0.4B 2.8B 71.9 - - 74.1 605 InternVL2-2B [16] 1.9B 0.3B 2.2B 86.9 76.2 58.9 73.4 784 Qwen2-VL-2B [88] 1.5B 0.7B 2.2B 90.1 73.5 65.5 79.7 794 MM 1.5-3B [107] - - 3B 87.7 74.2 58.5 76.5 657 DeepSeek-VL2-Small 2.4Bâ€ 0.4B 2.8Bâ€ 92.3 84.5 75.8 83.4 834 Open-source Model (4B - 13B) Phi-3.5-Vision [1] 3.8B 0.3B 4.1B 69.3 81.8 36.6 72.0 599 InternVL2-4B [16] 3.8B 0.3B 4.1B 89.2 81.5 67.0 74.4 788 Aria-MoE [46] 3.9Bâ€ 0.4B 4.3Bâ€ 92.6 86.4 - 81.1 - MM 1.5-7B [107] - - 7B 88.1 78.6 59.5 76.5 635 LLaVA-OV 7B [45] 7.6B 0.4B 8.0B 87.5 80.0 68.8 - - Molmo-7B-O [22] 7.3B 0.3B 7.6B - 80.4 70.0 80.4 - MiniCPM-V2.6 [99] 7.6B 0.4B 8.0B 90.8 82.4 - 80.1 852 (CoT) InternVL2-8B [16] 7.7B 0.3B 8.0B 91.6 83.3 74.8 77.4 794 Qwen2-VL-7B [88] 7.6B 0.7B 8.3B 94.5 83.0 76.5 84.3 845 Pixtral-12B [3] 12.0B 0.4B 12.4B 90.7 81.8 (CoT) 50.8 75.7 DeepSeek-VL 7B [59] 6.9B 0.4B 7.3B - - - - 456 DeepSeek-VL2 4.1Bâ€ 0.4B 4.5Bâ€ 93.3 86.0 78.1 84.2 811 for different kinds of data and switch between these two strategies on demand. The training of DeepSeek-VL2 was completed in 7/10/14 days using a cluster of 16/33/42 nodes, with each node equipped with 8 NVIDIA A100 GPUs. 5. Evaluation 5.1. Multimodal Performance Benchmarks We perform a holistic evaluation of DeepSeek-VL2 across a collection of com- monly used benchmarks, including DocVQA [ 66], ChartQA [ 65], InfoVQA2[67], TextVQA [ 77], RealWorldQA [ 95], OCRBench [ 57], AI2D [ 34], MMMU [ 105], MMStar [ 13], MathVista [ 60], MME [ 26], MMBench, MMBench-V1.1 [ 58] and MMT-Bench [ 100]. These benchmarks span diverse tasks from document understanding and chart interpretation to real-world problem solv- ing, enabling comprehensive evaluation of our modelâ€™s capabilities. To evaluate the grounding capability of our models, we test DeepSeek-VL2 on the RefCOCO, RefCOCO+ and RefCOCOg benchmarks [33, 64]. 2Given that InfoVQA contains images with extreme aspect ratios and excessively large images, we enlarge the candidate resolutions as ğ¶ğ‘…={(ğ‘šÂ·384,ğ‘›Â·384) |ğ‘šâˆˆN,ğ‘›âˆˆN, 1â‰¤ğ‘š,ğ‘›,ğ‘šğ‘›â‰¤18}when evaluating. 11 Table 4|Comparison with state-of-the-art models on general QA and math-related multimodal benchmarks .â€ : activated parameters of MoE model. *: evaluated in a different setting. Model#Params MMStar AI2D MMMU MME MMBench MMBench MMBench-V1.1 MMT-Bench RealWorldQA MathVista (Activated) (test) (val) (sum) (en test) (cn test) (testmini) Closed Model GPT-4V [69] - 56.0 89.4 63.1 1,927 81 80.2 80 64.3 61.4 58.1 GPT-4o [32] - 63.9 94.2 69.1 2,329 83.4 82.1 82.2 65.5 75.4 63.8 Claude 3.5 Sonnet [5] - 62.2 94.7 68.3 1,920 79.7 80.7 78.5 - 60.1 67.7 Gemini-1.5-Pro [81] - - 94.4 62.2 - - - - 64.5 70.4 63.9 Open-source Model (0.5B - 3B) LLaVA-OV 0.5B [45] 0.9B 37.7 57.1 31.4 1,478 61.6 55.5 59.6 - 55.6 34.8 InternVL2-1B

75.4 63.8 Claude 3.5 Sonnet [5] - 62.2 94.7 68.3 1,920 79.7 80.7 78.5 - 60.1 67.7 Gemini-1.5-Pro [81] - - 94.4 62.2 - - - - 64.5 70.4 63.9 Open-source Model (0.5B - 3B) LLaVA-OV 0.5B [45] 0.9B 37.7 57.1 31.4 1,478 61.6 55.5 59.6 - 55.6 34.8 InternVL2-1B [16] 0.9B 45.7 64.1 35.4 1,794 65.4 60.7 61.6 49.5 50.3 37.7 MM 1.5-1B [107] 1B - 59.3 35.8 1,611 - - - - 53.3 37.2 DeepSeek-VL2-Tiny 1.0Bâ€ 45.9 71.6 40.7 1,915 73.3 69.2 68.3 53.2 64.2 53.6 MolmoE-1B [22] 1.5Bâ€ - 86.4* 34.9 - - - - - 60.4 34 MiniCPM-V 2.0 [99] 2.8B - - 38.2 1,809 69.6 68.1 - - - 38.7 InternVL2-2B [16] 2.2B 49.8 74.1 36.3 1,877 73.2 70.9 69.6 50.4 57.3 46.3 Qwen2-VL-2B [88] 2.2B 48 74.4 41.1 1,872 74.9 73.5 72.2 54.5 62.9 47.8 MM 1.5-3B [107] 3B - 65.7 37.1 1,798 - - - - 56.9 44.4 DeepSeek-VL2-Small 2.8Bâ€ 57.0 80.0 48.0 2,123 82.3 80.3 79.3 62.9 65.4 60.7 Open-source Model (4B - 13B) Phi-3.5-Vision [1] 4.1B 47.5 78.1 43 - 76 66.1 72.1 53.6 53.6 43.9 InternVL2-4B [16] 4.1B 54.3 78.9 47.9 2,060 78.6 73.9 75.8 55.7 60.7 58.6 Aria-MoE [46] 4.3Bâ€ - - 54.9 - - - - - - 66.1 MM 1.5-7B [107] 7B - 72.2 41.8 1,861 - - - - 62.5 47.6 LLaVA-OV 7B [45] 8.0B - 81.4 48.8 1,998 80.8 - - - 66.3 63.2 Molmo-7B-O [22] 7.6Bâ€ - 90.7* 39.3 - - - - - 67.5 44.5 MiniCPM-V2.6 [99] 8.0B 57.5 82.1 49.8 (CoT) 2,348 (CoT) 81.5 79.3 78.0 60.8 65.0 60.6 InternVL2-8B [16] 8.0B 61.5 83.8 51.8 2,210 81.7 81.2 79.4 60.0 64.4 58.3 Qwen2-VL-7B [88] 8.3B 60.7 83 54.1 2,327 83 80.5 80.7 63.7 70.1 58.2 Pixtral-12B [3] 12.4B - - 52.5 (CoT) - - - - - 65.4 58 (CoT) DeepSeek-VL 7B [59] 7.3B - - 36.6 - 73.2 - - - - - DeepSeek-VL2 4.5Bâ€ 61.3 81.4 51.1 2,253 83.1 79.6 79.2 63.6 68.4 62.8 Comparison with the state-of-the-arts On the multimodal understanding benchmarks, we compare DeepSeek-VL2 with state-of-the-art models, including LLaVA-OV [ 45], InternVL2 [ 15], DeepSeek-VL [ 59], Qwen2-VL [ 88], Phi-3.5-Vision [ 1], Molmo [ 22], Pixtral [ 3], MM1.5 [ 107] and Aria-MoE [ 46]. The results are reported in Table 3 and 4. Benefited from our MoE architecture, DeepSeek-VL2 achieves similar or better performance with fewer activated parameters. On the grounding benchmarks, we compare DeepSeek-VL2 with Groudning DINO [ 56], UNINEXT [ 52], ONE-PEACE [ 87], mPLUG-2 [ 97], Florence-2 [ 96], InternVL2 [ 16], Shikra [ 11], TextHawk2 [ 103], Ferret-v2 [ 108], MM1.5 [ 107] and Qwen2 [ 88]. Our models outperforms the other VLMs at similar scales. 5.2. Qualitative Study In this section, we demonstrate different capabilities of DeepSeek-VL2, ranging from general question answering to visual storytelling and visual grounding. General visual question answering. Benefited from our new VL pretraining dataset and diverse SFT data. DeepSeek-VL2 demonstrated significantly improved ability on general visual question answering, as shown in Figure

Study In this section, we demonstrate different capabilities of DeepSeek-VL2, ranging from general question answering to visual storytelling and visual grounding. General visual question answering. Benefited from our new VL pretraining dataset and diverse SFT data. DeepSeek-VL2 demonstrated significantly improved ability on general visual question answering, as shown in Figure 4. Overall, this model excels at dense image description and it is able to recognize common landmarks, general visual knowledge, and rich-texts in both English and Chinese. It also performs favorably on chart understanding with accurate attributes recognition. Furthermore, we show the improved meme understanding of DeepSeek-VL2 in Figure 5, where it can describe the correct context and explain the humor with meaningful cultural background. 12 ImageDescription Describethissceneindetail.An expansive coastal landscape dominated by towering limestone stacks rising dramatically from turquoise waters of the ocean. These natural formations are part of The Twelve Apostles geological formation located on the Great Ocean Road in Victoria, Australia. In the foreground stands a prominent stack closest to the viewer's perspective; it is flanked by smaller stacks extending into the distance towards the horizon where they gradually diminish due to atmospheric haze or mist shrouding them partially. To the right side of the frame lies a sandy beach that curves gently around as if embracing the rugged cliffs which form the backdrop for this scene. The sky above appears overcast yet diffused sunlight illuminates parts of the cliff faces casting subtle shadows across some areas while highlighting others, adding depth and texture to the overall composition. Small waves break against both the base of the stacks and nearer shoreline creating white foam patterns contrasting sharply with the deep blue-green sea water below. Vegetation can also be seen at various points including atop certain stacks providing hints of greenery amidst predominantly earth-toned rock structures. This serene but powerful seascape captures natureâ€™s grandeur through its interplay between landforms, water bodies, light conditions, and vegetation all harmoniously coexisting within the same visual space. è¿™å¹…å›¾æç»˜äº†â¼€ä¸ªæ¹–è¾¹çš„â½‡è½æ™¯è±¡,å‘¨å›´æœ‰â¼­å³¦å’Œâ¼€äº›åœæ³Šåœ¨â½”â¾¯çš„èˆ¹åªã€‚ç‰¹åˆ«æ˜¾çœ¼çš„æ˜¯ä¸­é—´çš„â¼€è‰˜â¼¤èˆ¹,èˆ¹ä¸Šæœ‰ç€ç±»ä¼¼ä¸­å›½ä¼ ç»Ÿå»ºç­‘â»›æ ¼çš„å±‹æªã€‚è¿™ç§å»ºç­‘â»›æ ¼â¾®å¸¸åƒä¸­å›½å¤ä»£å»ºç­‘,å°¤å…¶æ˜¯çš‡å®¶æˆ–å¯ºåº™å»ºç­‘ã€‚è¿™ç§åœºæ™¯å¯èƒ½ä½äºä¸­å›½çš„æŸä¸ªè‘—åæ¹–æ³Š,ä¾‹å¦‚æ­å·çš„â»„æ¹–æˆ–å—äº¬çš„â½æ­¦æ¹–ç­‰ã€‚ç‰¹åˆ«æ˜¯å¦‚æœè¿™è‰˜èˆ¹æ˜¯ä»¿å¤å¼çš„æ¸¸èˆ¹,é‚£ä¹ˆå®ƒå¾ˆå¯èƒ½ä½äºâ»„æ¹–æ™¯åŒºã€‚â»„æ¹–æ˜¯ä¸­å›½æœ€è‘—åçš„æ¹–æ³Šä¹‹â¼€,å‘¨å›´æœ‰ç€ç§€ä¸½çš„â¼­æ™¯å’Œå¤å»ºç­‘,å¸¸å¸¸æˆä¸ºæ¸¸å®¢è§‚å…‰å’Œä½“éªŒä¼ ç»Ÿâ½‚åŒ–çš„åœ°â½…ã€‚å› æ­¤,å›¾â½šä¸­å±•ç¤ºçš„å¯èƒ½æ˜¯ä¸­å›½çš„â»„æ¹–,é‚£ä¸ªæœ‰ç€ä¸°å¯Œçš„å†å²â½‚åŒ–å’Œâ¾ƒç„¶ç¾æ™¯çš„åœ°â½…ã€‚çŒœçŒœè¿™æ˜¯å“ªé‡Œï¼Ÿ LandmarkRecognition è¯·ç»™è¿™å¹…æ™¯è±¡å†™â¼€â¾¸è¯—ã€Šæœ›å…¬â¼­ã€‹æµ®äº‘å‡ºå²«æ™“è½»å¯’,éƒéƒè‹â¼­å±•ç”»å±ã€‚â¼æ ‘å­¤å½±æ¥ç»¿é‡,â¼€æ³“æ·±â½”æŠ±â¾¦åŸã€‚é¸¿é¸£è¿œå²«åƒâ¼­èµ·,é¹¤ä¼´é—²å±…ä¸‡ç‰©å…´ã€‚å°½æœ›çƒŸéœâ¼­â¾Šç§€,å´å¯»æ˜¥â¾Šè¿‡æ±Ÿäº­ã€‚ PoetWriting The cake in the image is themed around the iconic cartoon series "Tom and Jerry." This series, created by William Hanna and Joseph Barbera, first aired in 1940 and became one of the most famous animated shows of all time. The story revolves around the never-ending chase between the cunning Tom the Cat and Jerry the Mouse. Each episode typically features Tom making various plans to catch Jerry, only to be outsmarted or thwarted by Jerry's cleverness and cunning. The dynamic duo is a symbol of constant rivalry and humor, and their interactions have been a part of popular culture for decades. This cake captures the essence of their playful and chaotic relationship through its design and decoration. Can you introduce the background story of this cake?GeneralKnowledge ChartUnderstanding Can you explain this chart to me?The chart shows a comparison of multilingual datasets based on the average normalized score across six languages: Arabic (ar), French (fr), Russian (ru), Thai (th), Turkish (tr), and Chinese (zh). The x-axis represents the number of training

the background story of this cake?GeneralKnowledge ChartUnderstanding Can you explain this chart to me?The chart shows a comparison of multilingual datasets based on the average normalized score across six languages: Arabic (ar), French (fr), Russian (ru), Thai (th), Turkish (tr), and Chinese (zh). The x-axis represents the number of training tokens in billions, while the y-axis shows the average normalized score, which is a measure of model performance.### Key Points:1. **Datasets Compared**: - FineWeb 2 (Ours) - CulturaX - HPLT-2 - CC-100 - CommonCrawl - mC4 - HPLT2. **Trend Analysis**: - FineWeb 2 consistently achieves the highest average normalized score, indicating superior performance compared to other datasets across the given range of training tokens. - The other datasets show varying levels of performance but tend to converge towards a similar score as the number of training tokens increases.3. **Performance Insights**: - Initially, FineWeb 2 has a significant advantage over other datasets, which highlights the effectiveness of the data collected in this dataset. - As training tokens increase, the performance gap between datasets narrows, suggesting diminishing returns in terms of dataset size. - FineWeb 2 demonstrates robustness, maintaining a lead over the other datasets throughout the training process.### Interpretation:The chart underscores the importance of dataset quality and size in achieving high model performance in multilingual tasks. The superior results of FineWeb 2 indicate its potential to enhance language model training for various applications across multiple languages. å›¾ä¸­åœ¨å®£ä¼ ä»€ä¹ˆä¸œâ»„ï¼ŸServing Soul Since Twenty Twelve TextRecognition What's written on the large black board. å›¾ä¸­å±•ç¤ºçš„æ˜¯ä¸â»„æ¹–é†‹â»¥ç›¸å…³çš„â»å“å’Œé¥®å“ã€‚å·¦è¾¹çš„â¼´å‘Šç‰Œä¸Šå±•ç¤ºçš„æ˜¯â»„æ¹–é†‹â»¥è›‹ç³•ï¼ˆWest Lake Fish Cakeï¼‰ï¼Œå®ƒæ˜¯â¼€ç§ä»¥â»„æ¹–é†‹â»¥ä¸ºä¸»é¢˜çš„â»å“ã€‚å³è¾¹çš„â¼´å‘Šç‰Œåˆ™æ˜¯åœ¨å®£ä¼ â»„æ¹–é†‹â»¥å’–å•¡ï¼Œè¿™ç§å’–å•¡è¢«ç§°ä¸ºâ€œé“ç‹—å’–å•¡â€ï¼Œå…¶ç‰¹â¾Šæ˜¯å°†â»„æ¹–é†‹â»¥çš„ç‰¹â¾Šå’Œæ­å·å½“åœ°çš„ç»éªŒç»“åˆåœ¨â¼€èµ·ï¼ŒæŠŠâ»„æ¹–é†‹â»¥çš„å‘³é“èâ¼Šåˆ°å’–å•¡ä¸­ã€‚â»„æ¹–é†‹â»¥æ˜¯æ­å·çš„â¼€é“è‘—åç¾â»ï¼Œä¸»è¦ä½¿â½¤è‰â»¥åˆ¶ä½œï¼Œç»è¿‡æ–™é…’ã€é†‹ã€ç³–ç­‰è°ƒæ–™çƒ¹é¥ªï¼Œé…¸ç”œå¯â¼ã€‚ç°åœ¨é€šè¿‡å°†â»„æ¹–é†‹â»¥åŠ â¼Šåˆ°å…¶ä»–â»å“å’Œé¥®å“ä¸­ï¼Œè¿™â¼€ä¼ ç»Ÿç¾â»å¾—åˆ°äº†å…¨æ–°çš„è¯ é‡Šã€‚ Figure 4|General question answering capability of DeepSeek-VL2 . Our model has multi- faceted capabilities, including recognition of landmarks, image-based poetry composition, answering questions about general knowledge, understanding charts, recognizing text, and more. 13 è¿™å¼ æ¢—å›¾å±•ç¤ºäº†â¼€ä½â½¼â¼ˆåœ¨æµ·è¾¹ï¼Œä¼¼ä¹æ­£åœ¨è¯•å›¾â½¤é”¤â¼¦å°†â¼€äº›ä¸œâ»„æ‰“è¿›æ²™æ»©çš„æƒ…æ™¯ã€‚ä¸Šâ½…é…â½‚æ˜¯â€œè¯•å›¾æŒ½ç•™å‡æœŸçš„æˆ‘â€ï¼Œæ˜¾ç„¶å°†â½¼â¼ˆçš„â¾ä¸ºä¸â€œç•™ä¸ä½â€å‡æœŸçš„æƒ…å†µè”ç³»åœ¨â¼€èµ·ã€‚### åˆ†æä¸è§£è¯»ï¼š1. **å›¾åƒæƒ…æ™¯ä¸åŠ¨ä½œå¯¹â½çš„å¹½é»˜æ€§**ï¼š- â½¼â¼ˆçš„åŠ¨ä½œæ˜¾å¾—â¼—åˆ†å¾’åŠ³ï¼Œå› ä¸ºæ²™æ»©ä¸â½”è¾¹çš„æ²™â¼¦éš¾ä»¥å›ºå®šï¼Œä»–è¯•å›¾â½¤é”¤â¼¦é’‰ä½ä»€ä¹ˆï¼Œè¿™æ˜¾ç„¶æ˜¯â¼€ä»¶â½†æ³•å®ç°çš„äº‹æƒ…ã€‚è¿™å½¢è±¡åœ°è¡¨ç°äº†â¼€ä¸ªâ¼ˆåœ¨â¾¯å¯¹ä¸å¯æŠ—æ‹’çš„â¼’é‡ï¼ˆå¦‚å‡æœŸç»“æŸï¼‰æ—¶çš„â½†å¥ˆå’Œâ½†åŠ©ã€‚2. **â½‚å­—ä¸ç”»â¾¯çš„ç»“åˆ**ï¼š- â€œè¯•å›¾æŒ½ç•™å‡æœŸçš„æˆ‘â€ä¸å›¾â½šå½¢æˆç›´æ¥å…³è”ï¼Œç”»â¾¯å±•ç°äº†â¼€ç§æ»‘ç¨½çš„ã€ç•¥å¸¦å¤¸å¼ çš„å°è¯•æŒ½ç•™å‡æœŸçš„æƒ…æ™¯ã€‚â¼ˆä»¬åœ¨â½£æ´»ä¸­ä¹Ÿå¸¸å¸¸è¯•å›¾é˜»â½Œå³å°†ç»“æŸçš„å‡æœŸï¼Œâ½å¦‚è¯•å›¾åœ¨å‡æœŸæœ€åâ¼å¤©æ‹–å»¶ï¼Œæ¨è¿Ÿå½’ç¨‹ç­‰ã€‚è¿™ä¸ªæ¢—å°†è¿™ç§â¼¼ç†çŠ¶æ€ä»¥å¹½é»˜çš„â½…å¼è¡¨ç°å‡ºæ¥ã€‚3. **â½‚åŒ–èƒŒæ™¯**ï¼š- å‡æœŸç»“æŸæ˜¯å¾ˆå¤šç°ä»£â¼¯ä½œæ—çš„å…±åŒæ„Ÿå—ï¼Œå°¤å…¶æ˜¯â¾¯ä¸´èŠ‚åç»¼åˆç—‡ï¼Œâ¼ˆä»¬ä¼šæ„Ÿåˆ°â¾®å¸¸ç„¦è™‘ã€ä¸é€‚åº”ï¼Œå› æ­¤å›¾ä¸­çš„å¤¸å¼ è¡¨ç°â½…å¼â¾®å¸¸è´´è¿‘â¼ˆä»¬çœŸå®çš„æƒ…æ„Ÿä½“éªŒï¼Œå…·æœ‰è¾ƒå¼ºçš„å…±é¸£æ€§ã€‚### æ€»ç»“ï¼šè¿™å¼ æ¢—å›¾é€šè¿‡å›¾åƒå’Œâ½‚å­—çš„ç»“åˆï¼Œåˆ©â½¤â¼€ç§å¹½é»˜å’Œå¤¸å¼ çš„â¼¿æ³•ï¼Œè¡¨è¾¾äº†â¼ˆä»¬åœ¨å‡æœŸç»“æŸæ—¶è¯•å›¾â€œæŒ½ç•™â€å‡æœŸçš„â½†å¥ˆå’Œâ¼¼ç†æŒ£æ‰ã€‚å®ƒåæ˜ äº†â¼€ç§æ™®éçš„æƒ…æ„Ÿï¼Œå°¤å…¶æ˜¯åœ¨ç°ä»£å¿«èŠ‚å¥çš„â½£æ´»ä¸­ï¼Œå‡æœŸæ˜¯â¼ˆä»¬éš¾å¾—çš„æ”¾æ¾æ—¶é—´ï¼Œå½“å‡æœŸå³å°†ç»“æŸæ—¶ï¼Œâ¼ˆä»¬å¾€å¾€æ„Ÿåˆ°ç„¦è™‘å’Œä¸èˆã€‚è¯¦ç»†è§£é‡Šâ¼€ä¸‹è¿™å¼ æ¢—å›¾ The image humorously depicts the contrasting thoughts of a PhD student. On the left, the student is working at a desk, surrounded by books and papers, dreaming of a relaxing beach vacation. On the right, the student is actually at the beach, but their mind is still consumed by thoughts of work, represented by stacks of papers and a laptop. This illustrates the common experience of PhD students who often find it difficult to fully disconnect from their research, even when they are physically away from their work environment. What does this image imply? Memeunderstanding Explain this meme to me. This image depicts a humorous situation involving a young child and a cake. The child, appearing determined and mischievous, is depicted biting into a large chocolate cake on a table. The text at the top of the image reads: â€œMom told me not to touch it.â€### Analysis and Meaning:1. **Disobedience theme**: The text implies that the child has intentionally ignored the instruction or warning given by their mom. This reflects the common tendency of children to follow rules reluctantly and test boundaries when told otherwise.2. **Humor in contrast**: The exaggerated act of biting the cake makes the situation absurd, adding to the humor. The over-the-top behavior

child has intentionally ignored the instruction or warning given by their mom. This reflects the common tendency of children to follow rules reluctantly and test boundaries when told otherwise.2. **Humor in contrast**: The exaggerated act of biting the cake makes the situation absurd, adding to the humor. The over-the-top behavior contrasts with the seemingly simple instruction, creating an unexpected and funny outcome.3. **Relatable scenario**: This image humorously captures a widely understood, everyday behaviorâ€”children wanting things they are told they cannot have, leading to playful and sometimes rebellious actions.4. **Visual cue**: The child's action of taking a bite adds a dynamic and engaging element to the humor, amplifying the sense of mischief and delight in the moment.In short, this image humorously captures the playful defiance of childhood, using the visual of a child taking a bite of an untouchable cake to amplify the humor and relatability. è¿™å¼ å›¾ä¸ºä»€ä¹ˆå¥½ç¬‘è¿™å¼ å›¾æ˜¯â¼€ç§å¸¸â»…çš„å¹½é»˜æ¢—å›¾ï¼Œè¡¨è¾¾äº†åœ¨è€ƒè¯•æ—¶å­¦â½£å¯èƒ½ä¼šæ„Ÿå—åˆ°çš„â€œâ½†â¼’æ„Ÿâ€æˆ–â€œæ‰“å‡»æ„Ÿâ€ã€‚### å›¾ä¸­å«ä¹‰ï¼š1. **ä¸Šâ½…**ï¼šâ€œå½“ä½ ä¸ºè€ƒè¯•åšâ¾œäº†åŠŸè¯¾â€ï¼Œè¡¨ç¤ºå­¦â½£åœ¨â¾¯å¯¹è€ƒè¯•æ—¶â¾ƒè®¤ä¸ºå·²ç»åšå¥½å……åˆ†çš„å‡†å¤‡ï¼Œå……æ»¡ä¿¡â¼¼ã€‚2. **ä¸‹â½…**ï¼šâ€œç¬¬â¼€é¢˜ï¼šâ€æ­é…å›¾â½šä¸­çš„éª‘â¼ è¢«å°„ä¸­çœ¼ç›çš„åœºæ™¯ï¼Œè±¡å¾è€ƒè¯•å¼€å§‹åï¼Œç¬¬â¼€é¢˜çš„å†…å®¹è¶…å‡ºäº†â¾ƒâ¼°å‡†å¤‡çš„èŒƒå›´æˆ–éš¾åº¦ï¼Œç¬é—´è®©â¼ˆé™·â¼Šè¿·èŒ«ï¼Œä¹Ÿå¯èƒ½å¯¹è€ƒè¯•ä¿¡â¼¼é€ æˆé‡â¼¤æ‰“å‡»ã€‚### èƒŒæ™¯å’Œâ½‚åŒ–ï¼š1. **ä¸­ä¸–çºªéª‘â¼ ä¸æ­¦â¼ å½¢è±¡**ï¼šéª‘â¼ å’Œæ­¦â¼ é€šå¸¸è¢«â½¤æ¥è¡¨ç°å¼ºâ¼¤ã€â½†ç•çš„å½¢è±¡ï¼Œä½†åœ¨è¿™â¾¥ï¼Œéª‘â¼ è¢«ç®­å°„ä¸­ï¼Œé€ æˆäº†åå·®æ„Ÿï¼Œå¢å¼ºäº†å¯¹è€ƒè¯•çš„è°ƒä¾ƒæ•ˆæœã€‚2. **è€ƒè¯•çš„æ™®éæ€§**ï¼šè€ƒè¯•å¯¹å­¦â½£æ¥è¯´æ˜¯â½£æ´»ä¸­çš„é‡è¦éƒ¨åˆ†ï¼Œæ‰€ä»¥è¿™ç§å¤¸å¼ çš„å¯¹â½èƒ½å¼•èµ·â¼¤å®¶å…±é¸£ã€‚3. **è°ƒä¾ƒè€ƒè¯•éš¾åº¦**ï¼šç¬¬â¼€é¢˜é€šå¸¸éš¾åº¦è¾ƒâ¾¼ï¼Œæˆä¸ºè€ƒâ½£åœ¨è€ƒè¯•åˆæœŸæœ€å¯èƒ½å—åˆ°å†²å‡»çš„éƒ¨åˆ†ï¼Œè¿™ç§â€œç¬¬â¼€é¢˜å¿…è¶…çº²â€çš„æ—¢è§†æ„Ÿâ¾®å¸¸æ™®éã€‚### ç»“è®ºï¼šè¿™å¼ å›¾é€šè¿‡æˆå‰§åŒ–çš„åå·®ï¼Œå¹½é»˜åœ°è®½åˆºäº†åœ¨è€ƒè¯•ä¸­â¾¯å¯¹æŒ‘æˆ˜æ—¶çš„â½­ç›¾â¼¼ç†ï¼Œå¾ˆå®¹æ˜“è®©ç»å†è¿‡ç±»ä¼¼æƒ…æ™¯çš„â¼ˆä¼šâ¼¼â¼€ç¬‘ã€‚ Figure 5|Meme understanding capability of DeepSeek-VL2 . Our model can understand the humor in memes and provide explanations. 14 Multi-imageconversation å½“ç„¶å¯ä»¥ï¼è¿™ä¸ªâ»è°±ä½¿â½¤äº†èƒ¡èâ¼˜ã€â½Ÿâ½¶å’ŒçŒªè‚‹â»£ï¼Œå¯ä»¥åšå‡ºçš„å‘³é“â¾®å¸¸ä¸°å¯Œã€‚ä½ å¯ä»¥å°è¯•è¿™é“èƒ¡èâ¼˜â½Ÿâ½¶çŒªè‚‹â»£é”…ã€‚### èƒ¡èâ¼˜â½Ÿâ½¶çŒªè‚‹â»£é”…#### ææ–™ï¼š- çŒªè‚‹â»£500å…‹- èƒ¡èâ¼˜3æ ¹- â½Ÿâ½¶2æ ¹- è‘±é€‚é‡- å§œé€‚é‡- è’œé€‚é‡- â¼‹â»†1é¢—- æ–™é…’2æ±¤åŒ™- é…±æ²¹1æ±¤åŒ™- ç³–1æ±¤åŒ™- ç›é€‚é‡- é¸¡ç²¾æˆ–å‘³ç²¾ï¼ˆå¯é€‰ï¼‰#### æ­¥éª¤ï¼š1. **å¤„ç†â»æï¼š**- çŒªè‚‹â»£æ´—å‡€ååˆ‡å—ï¼Œâ½¤æ¸…â½”æ³¡30åˆ†é’Ÿå»â¾â½”ï¼Œæå‡ºå¤‡â½¤ã€‚- èƒ¡èâ¼˜å»â½ªï¼Œåˆ‡æˆå—ï¼›â½Ÿâ½¶å‰¥â½ªï¼Œåˆ‡æˆæ®µã€‚2. **ç„¯â½”ï¼š**- çŒªè‚‹â»£æ”¾â¼Šé”…ä¸­ï¼ŒåŠ â¼Šæ¸…â½”å’Œå§œâ½šï¼Œçƒ§å¼€ç„¯â½”3åˆ†é’Ÿï¼Œå»é™¤æµ®æ²«ï¼Œæå‡ºå¤‡â½¤ã€‚3. **ç‚’â¾¹æ–™ï¼š**- é”…ä¸­ç•™å°‘é‡åº•æ²¹ï¼Œæ”¾â¼Šâ¼‹â»†çˆ†â¾¹ï¼Œç„¶ååŠ â¼Šè‘±å§œè’œç‚’â¾¹ã€‚4. **ç‚–ç…®ï¼š**- å€’â¼Šç„¯â½”åçš„çŒªè‚‹â»£ï¼Œç¿»ç‚’å‡åŒ€ï¼ŒåŠ â¼Šæ–™é…’ã€é…±æ²¹ã€ç³–ã€ç›ï¼Œå€’â¼Šé€‚é‡çƒ­â½”ï¼Œç›–é”…ç‚–ç…®45åˆ†é’Ÿï¼Œç›´åˆ°â¾ç†Ÿé€ã€‚5. **åŠ â¼Šé…èœï¼š**- ç‚–ç…®è¿‡ç¨‹ä¸­ï¼ŒåŠ â¼Šèƒ¡èâ¼˜å’Œâ½Ÿâ½¶ï¼Œç»§ç»­ç‚–ç…®20åˆ†é’Ÿï¼Œç›´åˆ°è”¬èœè½¯ç†Ÿã€‚6. **è°ƒå‘³ï¼š**- æœ€ååŠ â¼Šé¸¡ç²¾æˆ–å‘³ç²¾ï¼Œæ ¹æ®ä¸ªâ¼ˆâ¼å‘³è°ƒå‘³ï¼Œæ…æ‹Œå‡åŒ€ï¼Œå³å¯å‡ºé”…äº«â½¤ã€‚è¿™é“èƒ¡èâ¼˜â½Ÿâ½¶çŒªè‚‹â»£é”…ï¼Œå‘³é“ä¸°å¯Œï¼Œè¥å…»å‡è¡¡ï¼Œâ¾®å¸¸é€‚åˆå–œæ¬¢å°è¯•ä¸åŒâ¼æ„Ÿçš„æœ‹å‹ã€‚ èƒ½å¸®æˆ‘ç”¨è¿™å‡ ä¸ªé£Ÿæåšä¸€é“èœå—? I am having lunch now and I like sweet things. What would be a good drink to pair with my meal?A good drink to pair with your meal might be the milkshake from image 3. The sweetness of the milkshake can complement the flavors of your spaghetti dish. Figure 6|Multi-image conversation capability of DeepSeek-VL2 . Our model can understand the relationships between multiple images and do some reasoning based on them. 15 Can you help me write a shorttravel story based on these photos?Title: "A Voyage Through Time and Lightâ€The wait was a symphony of anticipation, each note a silent prayer to the gods of travel. I sat in the Paris Airport terminal, the word "BIENVENUE" etched in bold, inviting letters above me. The Eiffel Tower symbol floated serenely, a promise of what was to come. I reached into my bag, feeling the cool metal of my suitcase handle, and bit my lip, trying not to sink further into the comfort of the grey seats. The ceiling lights buzzed softly, casting a sterile glow that neither cheered nor soothed my nerves.I boarded the plane, the roar of engines a symphony of promise. As we soared above the clouds, the world below became a patchwork of lifeâ€”houses, roads, riversâ€”each a story waiting to be told. Then, the familiar spire of the Arc de Triomphe appeared, its grandeur a testament to human ambition. I watched, entranced, as the plane descended, the iconic archway a beacon of hope and history. The sky, a canvas of blue, welcomed us with a smile.Finally, we landed. The terminal buzzed with life, a cacophony of languages and laughter. I walked through the glass doors, the world outside a blur of green and grey. I found myself in a room, the Mona Lisa's eyes

history. The sky, a canvas of blue, welcomed us with a smile.Finally, we landed. The terminal buzzed with life, a cacophony of languages and laughter. I walked through the glass doors, the world outside a blur of green and grey. I found myself in a room, the Mona Lisa's eyes staring back at me, a mystery untouched by time. The golden frame shimmered, and I felt a connection, a fleeting moment of understanding. The reflection in the glass showed me a different perspective, a reminder that beauty is in the eye of the beholder.I left the room, the Mona Lisa's enigmatic smile still etched in my mind. The journey had been a dance of light and shadow, a symphony of moments that painted a picture of a city where every corner held a story. As I stepped outside, the Parisian night embraced me, a welcome that felt both ancient and new, just like the city itself. VisualStoryWriting â½¤è¿™äº›å›¾â½šè®²â¼€ä¸ªæ•…äº‹å§ã€Šé›ªå¤œçš„å¥‡é‡ã€‹å†¬â½‡çš„å¯’â»›åœ¨â¼­â¾•ä¸­ä½è¯­ï¼Œé›ªèŠ±è½»è½»é£˜è½åœ¨â½¼â¿å…ˆâ½£çš„è“â¾Šè¿å¸½å¤–å¥—ä¸Šã€‚ä»–è‚©ä¸Šçš„æŸ³æ¡ç¯®â¼¦â¾¥è£…æ»¡äº†çº¢è‹¹æœï¼Œä»¿ä½›æ˜¯è¿™ä¸ªå¯’å†·ä¸–ç•Œä¸­å”¯â¼€çš„æ¸©æš–ã€‚â½¼â¿å…ˆâ½£å¾®ç¬‘ç€ï¼Œå°½ç®¡å¤©â½“å¯’å†·ï¼Œä»–çš„â¼¼ä¸­å´å……æ»¡äº†æœŸå¾…ã€‚ä»–çŸ¥é“ï¼Œä»Šæ™šå°†æ˜¯â¼€ä¸ªä¸å¹³å‡¡çš„å¤œæ™šã€‚å¤œå¹•é™ä¸´ï¼Œâ½¼â¿å…ˆâ½£â¾›è¿›äº†â¼€â½šç¥ç§˜çš„ç«ç‘°èŠ±å›­ã€‚ä¸­å¤®é‚£æœµç²‰çº¢â¾Šçš„ç«ç‘°ï¼ŒèŠ±ç“£åœ¨â½‰å…‰ä¸‹é—ªçƒç€æŸ”å’Œçš„å…‰èŠ’ã€‚â½¼â¿å…ˆâ½£è½»è½»è§¦æ‘¸èŠ±ç“£ï¼Œçªç„¶ï¼ŒèŠ±å›­ä¸­çš„â¼€åˆ‡éƒ½é™â½Œäº†ï¼Œåªæœ‰ä»–â¼¼ä¸­é‚£è‚¡è«åçš„â¼’é‡åœ¨æ¶ŒåŠ¨ã€‚å°±åœ¨è¿™æ—¶ï¼Œâ¼€ä½â½©å‘è‹è‹çš„å·«å¸ˆå‡ºç°åœ¨ä»–â¾¯å‰ã€‚å·«å¸ˆâ¼¿ä¸­çš„â½•çƒé—ªçƒç€æ©™â»©â¾Šçš„å…‰èŠ’ï¼Œä»¿ä½›åœ¨è¯‰è¯´ç€å¤â½¼çš„é­”æ³•ã€‚å·«å¸ˆå‘Šè¯‰â½¼â¿å…ˆâ½£ï¼Œè¿™â½šèŠ±å›­éšè—ç€â¼€ä¸ªç§˜å¯†ï¼Œåªæœ‰çœŸæ­£çš„å‹‡â¼ æ‰èƒ½è§£å¼€ã€‚â½¼â¿å…ˆâ½£â¿èµ·å‹‡â½“ï¼Œè·Ÿéšå·«å¸ˆæ¥åˆ°äº†â¼€â½šæ²æµ´åœ¨â¾¦â¾Šé˜³å…‰ä¸­çš„â½¥é‡ã€‚â¾¼â¼¤çš„æ ‘å½±åœ¨é˜³å…‰ä¸‹æ˜¾å¾—æ ¼å¤–ç¥ç§˜ã€‚å·«å¸ˆå°†â½•çƒæ”¾åœ¨æ ‘ä¸‹ï¼Œç¬é—´ï¼Œæ ‘â¼²ä¸­æ¶Œå‡ºäº†â¼€è‚¡æ¸©æš–çš„â¼’é‡ã€‚â½¼â¿å…ˆâ½£æ„Ÿåˆ°â¼€è‚¡â¼’é‡ä»â¼¼åº•æ¶Œå‡ºï¼Œä»–çŸ¥é“ï¼Œè¿™æ˜¯â¼¤â¾ƒç„¶çš„é¦ˆèµ ã€‚å½“å¤œå¹•å†æ¬¡é™ä¸´ï¼Œâ½¼â¿å…ˆâ½£å¸¦ç€æ»¡ç¯®çš„çº¢è‹¹æœå›åˆ°äº†â¼­â¾•ã€‚ä»–å¾®ç¬‘ç€ï¼Œâ¼¼ä¸­å……æ»¡äº†æ»¡â¾œä¸å¸Œæœ›ã€‚è¿™ä¸ªå†¬å¤©ï¼Œä»–ä¸ä»…æ‰¾åˆ°äº†æ¸©æš–ï¼Œè¿˜å‘ç°äº†å†…â¼¼çš„â¼’é‡ã€‚ Figure 7|Visual storytelling capability of DeepSeek-VL2 . Our model can accept multiple images as input and narrate a story in either Chinese or English based on the images. 16 Table 5|Comparison with state-of-the-art models on visual grounding benchmarks . Our models of different sizes have all achieved the best results among MLLMs with similar sizes. ModelRefCOCO RefCOCO+ RefCOCOg val testA testB val testA testB val test Vision Model Grounding DINO-Tiny [56] 89.2 91.9 86.0 81.1 87.4 74.7 85.2 84.9 Grounding DINO-Largey [56] 90.6 93.2 88.2 82.8 89.0 75.9 86.1 87.0 UNINEXT-H [52] 92.6 94.3 91.5 85.2 89.6 79.8 88.7 89.4 VLM + Task-Specific Fine-T uning ONE-PEACE [87] 92.6 94.2 89.3 88.8 92.2 83.2 89.2 89.3 mPLUG-2 [97] 90.1 92.8 86.1 - - 86.1 84.7 85.1 Florence-2-B [96] 92.6 94.8 91.5 86.8 91.7 82.2 89.8 82.2 Florence-2-L [96] 93.4 95.3 92.0 88.3 92.9 83.6 91.2 91.7 Open-source VLM (0.5B - 3B) InternVL2-1B [16] 83.6 88.7 79.8 76.0 83.6 67.7 80.2 79.9 DeepSeek-VL2-Tiny 84.7 87.8 78.4 75.9 83.9 67.4 73.8 83.9 InternVL2-2B [16] 82.3 88.2 75.9 73.5 82.8 63.3 77.6 78.3 DeepSeek-VL2-Small 93.9 95.3 91.3 89.4 92.9 84.8 92.6 92.6 Open-source VLM (4B - 9B) Shikra-7B [11] 87.0 90.6 80.2 81.6 87.4 72.1 82.3 82.2 TextHawk2-7B [103] 91.9 93.0 87.6 86.2 90.0 80.4 88.2 88.1 Ferret-v2-7B [108] 92.8 94.7 88.7 87.4 92.8 79.3 89.4 89.3 InternVL2-8B [16] 87.1 91.1 80.7 79.8 87.9 71.4 82.7 82.7 MM1.5-7B [107] - 92.5 86.7 - 88.7 77.8 - 87.1 Qwen2-VL-7B [88] 91.7 93.6 87.3 85.8 90.5 79.5 87.3 87.8 DeepSeek-VL2 95.1 96.7 92.7 91.2 94.9 87.4 92.8 92.9 Multi-image conversation. DeepSeek-VL2 demonstrated improved ability on multi-image conversation, as shown in Figure 6. Our model can analyze the associations and differences among multiple images, while also enabling simple reasoning by integrating the content of several images. For example, it can think about how to prepare a dish based on images of certain ingredients. Visual storytelling. In Figure 7, we show DeepSeek-VL2 is able to write a creative story given a few images. The story writing is backed by its

by integrating the content of several images. For example, it can think about how to prepare a dish based on images of certain ingredients. Visual storytelling. In Figure 7, we show DeepSeek-VL2 is able to write a creative story given a few images. The story writing is backed by its strong general visual capabilities such as landmark recognition and OCR, as highlight in green texts. In addition, since the story writing ability is originally from the text-only DeepSeek Chat model, which is already aligned with good safety, we do not observe significant harmful and NSFW output from DeepSeek-VL2 during our internal testing. However, it is worth noting that creative storytelling in real-world scenarios demands more diverse genres ( e.g., horror, comedy, action) and varied plot types ( e.g., happy or tragic endings), which may inherently conflict with the safety requirements in LLM/VLM research. We aim to explore solutions to broaden the scope of storytelling while considering these challenges. Visual grounding. Visual grounding is a new ability we bring to DeepSeek-VL2. In Figure 8, we show the general grounding ability of DeepSeek-VL2. Interestingly, although the majority of images in our training set come from natural scenes, and the referring expressions are object category names or specific descriptions of objects, we find that the model is capable of generalizing to other scenarios (such as memes and animes), and has the ability to recognize certain celebrities and abstract concepts. Furthermore, we show DeepSeek-VL2 has in-context visual grounding ability in Figure 10. Given the first image, where an object is referred by the 17 NaturalScenes Find <|ref|>The DeepThink button<|/ref|>.WebUI/Screenshot æ‰¾åˆ°<|ref|>æ·¡å®šå§<|/ref|>ã€‚ <|ref|>ActionGames<|/ref|>Pinpoint <|ref|>Notifications<|/ref|> in the image with its coordinates. <|ref|>å­™æ‚Ÿç©º<|/ref|>Meme/AnimeLocate the <|ref|>space needle<|/ref|> in the given image. æ‰¾åˆ°æ‰€æœ‰çš„<|ref|>è‘«èŠ¦å¨ƒ<|/ref|>ã€‚<|ref|>Theloneliestman<|/ref|> <|ref|>Suricata suricatta<|/ref|>Find all the <|ref|>Watermelon slices<|/ref|> Identify <|ref|>Albert Einstein<|/ref|> in thispicture. Identify all objects in the image and output them in bounding boxes.<|ref|>The giraffe at the back.<|/ref|> Figure 8|Visual grounding ability of DeepSeek-VL2 . Our model can locate objects based on their category names, descriptions, or some abstract concepts. 18 GroundedConversation <|grounding|>If you feel hot, what will you do?To cool down, you can use <|ref|>thefan<|/ref|><|det|>[[166, 460, 338, 712]]<|/det|>which is sitting on the desk.<|grounding|> I'm a fig lover, which one would you recommend?If you're a fig lover, you should definitely try <|ref|>the green cake<|/ref|><|det|>[[635, 299, 951, 690]]<|/det|> . It's a matcha fig cake, which is a new addition to their menu. Well, you'll want to use the <|ref|>trashcan<|/ref|><|det|>[[583, 639, 811, 994]]<|/det|> that's positioned on the sidewalk. <|grounding|>I want to throw out the trash now, what should I do? <|grounding|>How to maintainworld peace? It's a big question, but one way could be by not having destructive weapons like </ref|>the big bomb</ref|><|det|>[[53, 997,932]]</det> on display in museums. Figure 9|Grounded conversation with DeepSeek-VL2 . Our model can perform reasoning on images while identifying the locations of relevant objects, thereby enabling the possibility of interacting with the real world. In-contextVisualGrounding <|grounding|>In the first image, an object within the red rectangle is marked. Locate the object of the same category in the second

with DeepSeek-VL2 . Our model can perform reasoning on images while identifying the locations of relevant objects, thereby enabling the possibility of interacting with the real world. In-contextVisualGrounding <|grounding|>In the first image, an object within the red rectangle is marked. Locate the object of the same category in the second image. <|grounding|> Find the most frequently appearing fruit from the first image in the second image. <|grounding|>According to the first image, which dish contains that ingredient in the second image? Figure 10|In-context visual grounding with DeepSeek-VL2 . Given one image, either with or without visual prompts, DeepSeek-VL2 is able to find relevant objects in another image. 19 visual prompt, the model is able to locate the object of the same category in the second image. We also observe that the model has exhibited emergent abilities. Given an image and textual descriptions, the model can combine the information from the image and the text to identify the corresponding object in a second image. Examples are listed in the second and the third rows in Figure 10. Grounding conversation. With the special token <|grounding|> , DeepSeek-VL2 can unleash its ability of grounded conversation, where it can refer to the key objects with accurate locations in its response, as demonstrated in Figure 9. This enables the model to interact better with the real world, thereby creating opportunities to play a greater role in fields such as embodied AI and computer/phone agents. 6. Conclusion In this technical report, we introduce DeepSeek-VL2, an enhanced version of MoE-based Vision- Language Models, available in scales of 3B, 16B, and 27B parameters in total, with corresponding activated parameters of 1.0B, 2.8B, and 4.5B. This configuration facilitates efficient computational consumption during both training and inference stages. Notably, our 3B, 16B and 27B models can be deployed on a single GPU with 10 GB, 40GB and 80GB memory respectively. We employ a dynamic tiling vision encoding strategy to efficiently process high-resolution images with various aspect ratios. By making codes and pre-trained models publicly available, we aim to stimulate further advancements and applications at the intersection of vision and language. Limitations and Future Work While DeepSeek-VL2 demonstrates strong capabilities across various tasks, there are several areas for future improvements. Currently, DeepSeek-VL2â€™s context window only allows for a few images per chat session. We plan to extend the context window in our next version to enable richer multi-image interactions. Moreover, like other current VLMs, the model occasionally faces challenges with blurry images or unseen objects, presenting opportunities for improved robustness in future versions. Finally, while DeepSeek- VL2 excels in visual perception and recognition tasks, we aim to strengthen its reasoning capabilities. These identified areas guide our ongoing research directions as we continue to advance the modelâ€™s capabilities. 20 References [1]M. Abdin, J. Aneja, H. Awadalla, A. Awadallah, A. A. Awan, N. Bach, A. Bahree, A. Bakhtiari, J. Bao, H. Behl, et al. Phi-3 technical report: A highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. [2]agentsea. Wave-ui 25k. https://huggingface.co/datasets/agentsea/wave-u

to advance the modelâ€™s capabilities. 20 References [1]M. Abdin, J. Aneja, H. Awadalla, A. Awadallah, A. A. Awan, N. Bach, A. Bahree, A. Bakhtiari, J. Bao, H. Behl, et al. Phi-3 technical report: A highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. [2]agentsea. Wave-ui 25k. https://huggingface.co/datasets/agentsea/wave-u i-25k , 2024. [3]P . Agrawal, S. Antoniak, E. B. Hanna, B. Bout, D. Chaplot, J. Chudnovsky, D. Costa, B. De Monicault, S. Garg, T. Gervet, et al. Pixtral 12b. arXiv preprint arXiv:2410.07073 , 2024. [4]A. Amini, S. Gabriel, P . Lin, R. Koncel-Kedziorski, Y. Choi, and H. Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. arXiv preprint arXiv:1905.13319, 2019. [5]Anthropic. Claude 3.5 sonnet. https://www.anthropic.com/news/claude-3-5-s onnet , 2024. [6]Y. Bai, X. Du, Y. Liang, Y. Jin, Z. Liu, J. Zhou, T. Zheng, X. Zhang, N. Ma, Z. Wang, et al. Coig-cqia: Quality is all you need for chinese instruction fine-tuning. arXiv preprint arXiv:2403.18058, 2024. [7]L. Blecher. Latex-ocr â€” a tool to convert images of latex equations into latex code. https://github.com/lukas-blecher/LaTeX-OCR , 2023. Accessed: 2023-10-17. [8]O. B. Bohan and H. Face. Megalith 10m dataset. https://huggingface.co/dataset s/madebyollin/megalith-10m , 2024. [9]M. Cai, H. Liu, S. K. Mustikovela, G. P . Meyer, Y. Chai, D. Park, and Y. J. Lee. Vip-llava: Making large multimodal models understand arbitrary visual prompts. In CVPR , pages 12914â€“12923. IEEE, 2024. [10] G. H. Chen, S. Chen, R. Zhang, J. Chen, X. Wu, Z. Zhang, Z. Chen, J. Li, X. Wan, and B. Wang. Allava: Harnessing gpt4v-synthesized data for a lite vision-language model. arXiv preprint arXiv:2402.11684, 2024. [11] K. Chen, Z. Zhang, W. Zeng, R. Zhang, F. Zhu, and R. Zhao. Shikra: Unleashing multi- modal llmâ€™s referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023. [12] L. Chen, J. Li, X. Dong, P . Zhang, C. He, J. Wang, F. Zhao, and D. Lin. Sharegpt4v: Improving large multi-modal models with better captions. ECCV, 2023. [13] L. Chen, J. Li, X. Dong, P . Zhang, Y. Zang, Z. Chen, H. Duan, J. Wang, Y. Qiao, D. Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. [14] W. Chen, H. Wang, J. Chen, Y. Zhang, H. Wang, S. Li, X. Zhou, and W. Y. Wang. Tabfact: A large-scale dataset for table-based fact verification. In International Conference on Learning Representations. [15] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, M. Zhong, Q. Zhang, X. Zhu, L. Lu, B. Li, P . Luo, T. Lu, Y. Qiao, and J. Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. 21 [16] Z. Chen, W. Wang, H. Tian, S. Ye, Z. Gao, E. Cui, W. Tong, K. Hu, J. Luo, Z. Ma, et al. Internvl2: Better than the bestâ€”expanding performance boundaries of open-source multi- modal models with the progressive scaling strategy, 2024. [17] A. Cherian, K.-C. Peng, S. Lohit, K. Smith, and J. B. Tenenbaum. Are deep neural networks smarter than second graders?

Cui, W. Tong, K. Hu, J. Luo, Z. Ma, et al. Internvl2: Better than the bestâ€”expanding performance boundaries of open-source multi- modal models with the progressive scaling strategy, 2024. [17] A. Cherian, K.-C. Peng, S. Lohit, K. Smith, and J. B. Tenenbaum. Are deep neural networks smarter than second graders? arXiv preprint arXiv:2212.09993, 2022. [18] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [19] M. Conover, M. Hayes, A. Mathur, J. Xie, J. Wan, S. Shah, A. Ghodsi, P . Wendell, M. Zaharia, and R. Xin. Free dolly: Introducing the worldâ€™s first truly open instruction-tuned llm, 2023. URL https://www.databricks.com/blog/2023/04/12/dolly-first-ope n-commercially-viable-instruction-tuned-llm . [20] D. Dai, C. Deng, C. Zhao, R. Xu, H. Gao, D. Chen, J. Li, W. Zeng, X. Yu, Y. Wu, et al. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. arXiv preprint arXiv:2401.06066, 2024. [21] W. Dai, N. Lee, B. Wang, Z. Yang, Z. Liu, J. Barker, T. Rintamaki, M. Shoeybi, B. Catanzaro, and W. Ping. Nvlm: Open frontier-class multimodal llms. arXiv preprint, 2024. [22] M. Deitke, C. Clark, S. Lee, R. Tripathi, Y. Yang, J. S. Park, M. Salehi, N. Muennighoff, K. Lo, L. Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. [23] X. Deng, Y. Gu, B. Zheng, S. Chen, S. Stevens, B. Wang, H. Sun, and Y. Su. Mind2web: Towards a generalist agent for the web. Advances inNeural Information Processing Systems, 36, 2024. [24] M. Diem, S. Fiel, F. Kleber, R. Sablatnig, J. M. Saavedra, D. Contreras, J. M. Barrios, and L. S. Oliveira. Icfhr 2014 competition on handwritten digit string recognition in challenging datasets (hdsrc 2014). In 2014 14th International Conference onFrontiers inHandwriting Recognition, pages 779â€“784. IEEE, 2014. [25] B. Egan, A. Redden, XWAVE, and SilentAntagonist. Dalle3 1 Million+ High Quality Captions, May 2024. URL https://huggingface.co/datasets/ProGamerGov/sy nthetic-dataset-1m-dalle3-high-quality-captions . [26] C. Fu, P . Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin, J. Yang, X. Zheng, K. Li, X. Sun, Y. Wu, and R. Ji. Mme: A comprehensive evaluation benchmark for multimodal large language models, 2024. URL https://arxiv.org/abs/2306.13394 . [27] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh. Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering. In Conference onComputer Vision and Pattern Recognition (CVPR), 2017. [28] J. Gu, X. Meng, G. Lu, L. Hou, N. Minzhe, X. Liang, L. Yao, R. Huang, W. Zhang, X. Jiang, C. Xu, and H. Xu. Wukong: A 100 million large-scale chinese cross-modal pre-training benchmark. In NeurIPS, 2022. [29] C. He, Z. Jin, C. Xu, J. Qiu, B. Wang, W. Li, H. Yan, J. Wang, and D. Lin. Wanjuan: A comprehensive multimodal dataset for advancing english and chinese large models. arXiv preprint arXiv:2308.10755, 2023. 22 [30] High-flyer. HAI-LLM: Efficient and lightweight training tool for large models, 2023.

2022. [29] C. He, Z. Jin, C. Xu, J. Qiu, B. Wang, W. Li, H. Yan, J. Wang, and D. Lin. Wanjuan: A comprehensive multimodal dataset for advancing english and chinese large models. arXiv preprint arXiv:2308.10755, 2023. 22 [30] High-flyer. HAI-LLM: Efficient and lightweight training tool for large models, 2023. URL https://www.high-flyer.cn/en/blog/hai-llm . [31] D. A. Hudson and C. D. Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings oftheIEEE/CVF conference on computer vision and pattern recognition, pages 6700â€“6709, 2019. [32] A. Hurst, A. Lerer, A. P . Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Weli- hinda, A. Hayes, A. Radford, et al. Gpt-4v(ision) system card. 2023. [33] S. Kazemzadeh, V . Ordonez, M. Matten, and T. Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings ofthe2014 conference onempirical methods innatural language processing (EMNLP), pages 787â€“798, 2014. [34] A. Kembhavi, M. Salvato, E. Kolve, M. Seo, H. Hajishirzi, and A. Farhadi. A diagram is worth a dozen images. In Computer Visionâ€“ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11â€“14, 2016, Proceedings, Part IV14, pages 235â€“ 251. Springer, 2016. [35] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. White- head, A. C. Berg, W.-Y. Lo, et al. Segment anything. In Proceedings oftheIEEE/CVF International Conference onComputer Vision, pages 4015â€“4026, 2023. [36] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. White- head, A. C. Berg, W.-Y. Lo, et al. Segment anything. In Proceedings oftheIEEE/CVF International Conference onComputer Vision, pages 4015â€“4026, 2023. [37] Y. Kirstain, A. Polyak, U. Singer, S. Matiana, J. Penna, and O. Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. In NeurIPS, 2023. [38] M. Koupaee and W. Y. Wang. Wikihow: A large scale text summarization dataset. arXiv preprint arXiv:1810.09305, 2018. [39] A. Kuznetsova, H. Rom, N. Alldrin, J. Uijlings, I. Krasin, J. Pont-Tuset, S. Kamali, S. Popov, M. Malloci, A. Kolesnikov, T. Duerig, and V . Ferrari. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. IJCV , 2020. [40] LAION. Laion-aesthetics, 2023. URL https://laion.ai/blog/laion-aesthetics . Accessed: 2023-10-27. [41] H. LaurenÃ§on, L. Saulnier, L. Tronchon, S. Bekman, A. Singh, A. Lozhkov, T. Wang, S. Karamcheti, A. M. Rush, D. Kiela, M. Cord, and V . Sanh. OBELICS: an open web-scale filtered dataset of interleaved image-text documents. In NeurIPS, 2023. [42] H. LaurenÃ§on, A. Marafioti, V . Sanh, and L. Tronchon. Building and better understanding vision-language models: insights and future directions., 2024. [43] H. LaurenÃ§on, L. Tronchon, M. Cord, and V . Sanh. What matters when building vision- language models?, 2024. [44] H. LaurenÃ§on, L. Tronchon, and V . Sanh. Unlocking the conversion of web screenshots into html code with the websight dataset, 2024. [45] B. Li, Y. Zhang, D. Guo, R. Zhang, F. Li, H. Zhang, K. Zhang, P . Zhang, Y. Li, Z. Liu, et al. Llava-onevision: Easy visual task transfer. arXiv

L. Tronchon, and V . Sanh. Unlocking the conversion of web screenshots into html code with the websight dataset, 2024. [45] B. Li, Y. Zhang, D. Guo, R. Zhang, F. Li, H. Zhang, K. Zhang, P . Zhang, Y. Li, Z. Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 23 [46] D. Li, Y. Liu, H. Wu, Y. Wang, Z. Shen, B. Qu, X. Niu, G. Wang, B. Chen, and J. Li. Aria: An open multimodal native mixture-of-experts model. arXiv preprint arXiv:2410.05993 , 2024. [47] F. Li, R. Zhang, H. Zhang, Y. Zhang, B. Li, W. Li, Z. Ma, and C. Li. Llava-next- interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. [48] L. Li, Y. Wang, R. Xu, P . Wang, X. Feng, L. Kong, and Q. Liu. Multimodal ArXiv: A dataset for improving scientific comprehension of large vision-language models. In ACL, 2024. [49] L. Li, Y. Wang, R. Xu, P . Wang, X. Feng, L. Kong, and Q. Liu. Multimodal arxiv: A dataset for improving scientific comprehension of large vision-language models. arXiv preprint arXiv:2403.00231, 2024. [50] X. Li, F. Zhang, H. Diao, Y. Wang, X. Wang, and L.-Y. Duan. Densefusion-1m: Merging vision experts for comprehensive multimodal perception. arXiv preprint arXiv:2407.08303 , 2024. [51] Z. Li, X. Yang, K. Choi, W. Zhu, R. Hsieh, H. Kim, J. H. Lim, S. Ji, B. Lee, X. Yan, et al. Mmsci: A dataset for graduate-level multi-discipline multimodal scientific understanding. arXiv preprint arXiv:2407.04903, 2024. [52] F. Lin, J. Yuan, S. Wu, F. Wang, and Z. Wang. Uninext: Exploring a unified architecture for vision recognition. In Proceedings ofthe31st ACM International Conference on Multimedia, pages 3200â€“3208, 2023. [53] A. Liu, B. Feng, B. Wang, B. Wang, B. Liu, C. Zhao, C. Dengr, C. Ruan, D. Dai, D. Guo, et al. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434, 2024. [54] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. Advances inneural information processing systems, 36, 2023. [55] H. Liu, C. Li, Y. Li, B. Li, Y. Zhang, S. Shen, and Y. J. Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. URL https://llava-vl.github.io/blog /2024-01-30-llava-next/ . [56] S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, Q. Jiang, C. Li, J. Yang, H. Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. InEuropean Conference onComputer Vision, pages 38â€“55. Springer, 2025. [57] Y. Liu, Z. Li, B. Yang, C. Li, X. Yin, C.-l. Liu, L. Jin, and X. Bai. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2023. [58] Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European Conference on Computer Vision, pages 216â€“233. Springer, 2025. [59] H. Lu, W. Liu, B. Zhang, B. Wang, K. Dong, B. Liu, J. Sun, T. Ren,

Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European Conference on Computer Vision, pages 216â€“233. Springer, 2025. [59] H. Lu, W. Liu, B. Zhang, B. Wang, K. Dong, B. Liu, J. Sun, T. Ren, Z. Li, H. Yang, et al. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024. 24 [60] P . Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng, K.-W. Chang, M. Galley, and J. Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference onLearning Representations. [61] P . Lu, L. Qiu, J. Chen, T. Xia, Y. Zhao, W. Zhang, Z. Yu, X. Liang, and S.-C. Zhu. Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. arXiv preprint arXiv:2110.13214, 2021. [62] C. Ma, Y. Jiang, J. Wu, Z. Yuan, and X. Qi. Groma: Localized visual tokenization for grounding multimodal large language models. In European Conference onComputer Vision, pages 417â€“435. Springer, 2025. [63] Y. Ma, X. Liu, X. Chen, W. Liu, C. Wu, Z. Wu, Z. Pan, Z. Xie, H. Zhang, L. Zhao, et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal under- standing and generation. arXiv preprint arXiv:2411.07975, 2024. [64] J. Mao, J. Huang, A. Toshev, O. Camburu, A. L. Yuille, and K. Murphy. Generation and comprehension of unambiguous object descriptions. In Proceedings oftheIEEE conference oncomputer vision and pattern recognition, pages 11â€“20, 2016. [65] A. Masry, D. X. Long, J. Q. Tan, S. Joty, and E. Hoque. Chartqa: A benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244 , 2022. [66] M. Mathew, D. Karatzas, and C. Jawahar. Docvqa: A dataset for vqa on document images. InProceedings oftheIEEE/CVF winter conference onapplications ofcomputer vision , pages 2200â€“2209, 2021. [67] M. Mathew, V . Bagal, R. Tito, D. Karatzas, E. Valveny, and C. Jawahar. Infographicvqa. InProceedings oftheIEEE/CVF Winter Conference onApplications ofComputer Vision , pages 1697â€“1706, 2022. [68] A. Mitra, H. Khanpour, C. Rosset, and A. Awadallah. Orca-math: Unlocking the potential of slms in grade school math, 2024. [69] OpenAI. Gpt-4v(ision) system card. https://openai.com/research/gpt-4v-sys tem-card , 2023. [70] B. Peng, C. Li, P . He, M. Galley, and J. Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023. [71] Z. Peng, W. Wang, L. Dong, Y. Hao, S. Huang, S. Ma, and F. Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. [72] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo, J. Hockenmaier, and S. Lazeb- nik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image- to-sentence models. In Proceedings oftheIEEE international conference oncomputer vision, pages 2641â€“2649, 2015. [73] B. Saleh and A. Elgammal. Large-scale classification of fine-art paintings: Learning the right metric on the right feature. arXiv preprint arXiv:1505.00855, 2015. [74] S. Shah, A. Mishra, N. Yadati, and P . P . Talukdar. Kvqa: Knowledge-aware visual question answering. In Proceedings oftheAAAI conference onartificial intelligence ,

2641â€“2649, 2015. [73] B. Saleh and A. Elgammal. Large-scale classification of fine-art paintings: Learning the right metric on the right feature. arXiv preprint arXiv:1505.00855, 2015. [74] S. Shah, A. Mishra, N. Yadati, and P . P . Talukdar. Kvqa: Knowledge-aware visual question answering. In Proceedings oftheAAAI conference onartificial intelligence , volume 33, pages 8876â€“8884, 2019. 25 [75] S. Shao, Z. Li, T. Zhang, C. Peng, G. Yu, X. Zhang, J. Li, and J. Sun. Objects365: A large-scale, high-quality dataset for object detection. In Proceedings oftheIEEE/CVF international conference oncomputer vision, pages 8430â€“8439, 2019. [76] W. Shi, Z. Hu, Y. Bin, J. Liu, Y. Yang, S.-K. Ng, L. Bing, and R. K.-W. Lee. Math-llava: Bootstrapping mathematical reasoning for multimodal large language models. arXiv preprint arXiv:2406.17294, 2024. [77] A. Singh, V . Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach. To- wards vqa models that can read. In Proceedings oftheIEEE/CVF conference oncomputer vision and pattern recognition, pages 8317â€“8326, 2019. [78] V . Singla, K. Yue, S. Paul, R. Shirkavand, M. Jayawardhana, A. Ganjdanesh, H. Huang, A. Bhatele, G. Somepalli, and T. Goldstein. From pixels to prose: A large dataset of dense image captions. CoRR, abs/2406.10328, 2024. [79] K. Srinivasan, K. Raman, J. Chen, M. Bendersky, and M. Najork. Wit: Wikipedia-based im- age text dataset for multimodal multilingual machine learning. In SIGIR , page 2443â€“2449, 2021. [80] K. Sun, J. Pan, Y. Ge, H. Li, H. Duan, X. Wu, R. Zhang, A. Zhou, Z. Qin, Y. Wang, J. Dai, Y. Qiao, L. Wang, and H. Li. Journeydb: A benchmark for generative image understanding. InNeurIPS, 2023. [81] G. Team, P . Georgiev, V . I. Lei, R. Burnell, L. Bai, A. Gulati, G. Tanzer, D. Vincent, Z. Pan, S. Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [82] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland, D. Borth, and L.-J. Li. Yfcc100m: The new data in multimedia research. Communications oftheACM , 59(2): 64â€“73, 2016. [83] S. Tong, E. L. Brown II, P . Wu, S. Woo, A. J. IYER, S. C. Akula, S. Yang, J. Yang, M. Mid- depogu, Z. Wang, et al. Cambrian-1: A fully open, vision-centric exploration of multi- modal llms. In The Thirty-eighth Annual Conference onNeural Information Processing Systems. [84] S. Toshniwal, W. Du, I. Moshkov, B. Kisacanin, A. Ayrapetyan, and I. Gitman. Openmathinstruct-2: Accelerating ai for math with massive open-source instruction data. arXiv preprint arXiv:2410.01560, 2024. [85] J. Wang, P . Zhang, T. Chu, Y. Cao, Y. Zhou, T. Wu, B. Wang, C. He, and D. Lin. V3det: Vast vocabulary visual detection dataset. In Proceedings oftheIEEE/CVF International Conference onComputer Vision, pages 19844â€“19854, 2023. [86] L. Wang, H. Gao, C. Zhao, X. Sun, and D. Dai. Auxiliary-loss-free load balancing strategy for mixture-of-experts. CoRR , abs/2408.15664, 2024. doi: 10.48550/ARXIV.2408.15664. URL https://doi.org/10.48550/arXiv.2408.15664 . [87] P . Wang, S. Wang, J. Lin, S. Bai, X. Zhou, J. Zhou, X. Wang, and C. Zhou. One-peace:

pages 19844â€“19854, 2023. [86] L. Wang, H. Gao, C. Zhao, X. Sun, and D. Dai. Auxiliary-loss-free load balancing strategy for mixture-of-experts. CoRR , abs/2408.15664, 2024. doi: 10.48550/ARXIV.2408.15664. URL https://doi.org/10.48550/arXiv.2408.15664 . [87] P . Wang, S. Wang, J. Lin, S. Bai, X. Zhou, J. Zhou, X. Wang, and C. Zhou. One-peace: Exploring one general representation model toward unlimited modalities. arXiv preprint arXiv:2305.11172, 2023. 26 [88] P . Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge, et al. Qwen2-vl: Enhancing vision-language modelâ€™s perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [89] W. Wang, M. Shi, Q. Li, W. Wang, Z. Huang, L. Xing, Z. Chen, H. Li, X. Zhu, Z. Cao, et al. The all-seeing project: Towards panoptic visual recognition and understanding of the open world. arXiv preprint arXiv:2308.01907, 2023. [90] W. Wang, Y. Ren, H. Luo, T. Li, C. Yan, Z. Chen, W. Wang, Q. Li, L. Lu, X. Zhu, et al. The all-seeing project v2: Towards general relation comprehension of the open world. In European Conference onComputer Vision, pages 471â€“490. Springer, 2025. [91] Y. Wei, Z. Wang, J. Liu, Y. Ding, and L. Zhang. Magicoder: Empowering code genera- tion with OSS-instruct. In Proceedings ofthe41st International Conference onMachine Learning , volume 235 of Proceedings ofMachine Learning Research , pages 52632â€“52657. PMLR, 21â€“27 Jul 2024. URL https://proceedings.mlr.press/v235/wei24h.html . [92] C. Wendler. wendlerc/renderedtext. [93] C. Wendler. Renderedtext dataset. https://huggingface.co/datasets/wendlerc /RenderedText , 2023. Accessed: 2023-10-17. [94] C. Wu, X. Chen, Z. Wu, Y. Ma, X. Liu, Z. Pan, W. Liu, Z. Xie, X. Yu, C. Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024. [95] xAI. Grok-1.5 vision preview. 2024. [96] B. Xiao, H. Wu, W. Xu, X. Dai, H. Hu, Y. Lu, M. Zeng, C. Liu, and L. Yuan. Florence-2: Advancing a unified representation for a variety of vision tasks. In Proceedings ofthe IEEE/CVF Conference onComputer Vision and Pattern Recognition , pages 4818â€“4829, 2024. [97] H. Xu, Q. Ye, M. Yan, Y. Shi, J. Ye, Y. Xu, C. Li, B. Bi, Q. Qian, W. Wang, et al. mplug-2: A modularized multi-modal foundation model across text, image and video. In International Conference onMachine Learning, pages 38728â€“38748. PMLR, 2023. [98] Z. Xu, F. Jiang, L. Niu, Y. Deng, R. Poovendran, Y. Choi, and B. Y. Lin. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. arXiv preprint arXiv:2406.08464, 2024. [99] Y. Yao, T. Yu, A. Zhang, C. Wang, J. Cui, H. Zhu, T. Cai, H. Li, W. Zhao, Z. He, et al. Minicpm-v: A gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. [100] K. Ying, F. Meng, J. Wang, Z. Li, H. Lin, Y. Yang, H. Zhang, W. Zhang, Y. Lin, S. Liu, et al. Mmt-bench: A comprehensive multimodal benchmark for evaluating large vision- language models towards multitask agi. In Forty-first International Conference on Machine Learning. [101] L. Yu, P . Poirson, S. Yang, A. C. Berg, and

Wang, Z. Li, H. Lin, Y. Yang, H. Zhang, W. Zhang, Y. Lin, S. Liu, et al. Mmt-bench: A comprehensive multimodal benchmark for evaluating large vision- language models towards multitask agi. In Forty-first International Conference on Machine Learning. [101] L. Yu, P . Poirson, S. Yang, A. C. Berg, and T. L. Berg. Modeling context in referring expressions. In Computer Visionâ€“ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II14, pages 69â€“85. Springer, 2016. 27 [102] L. Yu, W. Jiang, H. Shi, J. Yu, Z. Liu, Y. Zhang, J. T. Kwok, Z. Li, A. Weller, and W. Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023. [103] Y.-Q. Yu, M. Liao, J. Zhang, and J. Wu. Texthawk2: A large vision-language model excels in bilingual ocr and grounding with 16x fewer tokens. arXiv preprint arXiv:2410.05261 , 2024. [104] Y. Yuan, X. Liu, W. Dikubab, H. Liu, Z. Ji, Z. Wu, and X. Bai. Syntax-aware network for handwritten mathematical expression recognition. arXiv preprint arXiv:2203.01601 , 2022. [105] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings oftheIEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 9556â€“9567, 2024. [106] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer. Sigmoid loss for language image pre- training. In Proceedings oftheIEEE/CVF International Conference onComputer Vision , pages 11975â€“11986, 2023. [107] H. Zhang, M. Gao, Z. Gan, P . Dufter, N. Wenzel, F. Huang, D. Shah, X. Du, B. Zhang, Y. Li, et al. Mm1.5: Methods, analysis & insights from multimodal llm fine-tuning. arXiv preprint arXiv:2409.20566, 2024. [108] H. Zhang, H. You, P . Dufter, B. Zhang, C. Chen, H.-Y. Chen, T.-J. Fu, W. Y. Wang, S.-F. Chang, Z. Gan, et al. Ferret-v2: An improved baseline for referring and grounding with large language models. arXiv preprint arXiv:2404.07973, 2024. [109] R. Zhang, X. Wei, D. Jiang, Y. Zhang, Z. Guo, C. Tong, J. Liu, A. Zhou, B. Wei, S. Zhang, P . Gao, and H. Li. Mavis: Mathematical visual instruction tuning, 2024. URL https: //arxiv.org/abs/2407.08739 . [110] B. Zheng, B. Gou, J. Kil, H. Sun, and Y. Su. Gpt-4v(ision) is a generalist web agent, if grounded. 2024. URL https://openreview.net/forum?id=piecKJ2DlB . [111] X. Zheng, D. Burdick, L. Popa, P . Zhong, and N. X. R. Wang. Global table extractor (gte): A framework for joint table identification and cell structure recognition using visual context. Winter Conference forApplications inComputer Vision (WACV), 2021. [112] X. Zhong, E. ShafieiBavani, and A. Jimeno-Yepes. Image-based table recognition: Data, model, and evaluation. In ECCV, volume 12366, pages 564â€“580, 2020. 28

