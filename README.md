# DeepSeeker-RAG

### Demo: https://huggingface.co/spaces/Roxanne-WANG/DeepSeeker_RAG

DeepSeeker-RAG is a Retrieval-Augmented Generation (RAG) system built from scratch to improve response performance in a specific domain—namely, academic research papers from the DeepSeek collection. This project integrates domain-specific knowledge into a large language model’s responses by retrieving relevant text chunks and augmenting the prompt with additional context.

## Objective

The primary objective of DeepSeeker-RAG is to enhance the quality and relevance of answers generated by a language model by:
- Extracting and processing text from DeepSeek PDF papers.
- Generating semantic embeddings for text chunks.
- Storing embeddings in a SQLite database.
- Retrieving relevant chunks based on a user query.
- Augmenting the query with contextual data and generating responses using OpenAI’s GPT‑3.5 Turbo model.

## Data

- **Source:** DeepSeek papers in PDF format.
- **Processing:**  
  - **Extraction:** Text is extracted from PDFs using a PDF parsing library.
  - **Preprocessing:** The extracted text is cleaned and normalized.
  - **Chunking:** Text is split into smaller segments (chunks) to fit within the context window of the language model.

## Pipeline

The complete processing pipeline consists of the following steps:

1. **Data Extraction:**  
   Convert PDFs to raw text using a dedicated extractor.

2. **Preprocessing & Chunking:**  
   Clean the text and split it into manageable chunks.

3. **Embedding Generation:**  
   Use the SentenceTransformer model (`all-MiniLM-L6-v2`) to generate dense, normalized embeddings for each text chunk.

4. **Storage:**  
   Store the text chunks and their corresponding embeddings in an SQLite database. This serves as our vector database for semantic retrieval.

5. **Semantic Retrieval:**  
   When a query is received, generate its embedding and compute cosine similarity with stored embeddings to retrieve the most relevant chunks.

6. **Prompt Construction & Response Generation:**  
   Combine the retrieved context with the original query to construct a prompt. Use OpenAI’s GPT‑3.5 Turbo (via ChatCompletion API) to generate a final answer.

7. **Frontend & Deployment:**  
   A user-friendly web interface is built using Streamlit. The entire application is deployed on the HuggingFace: https://huggingface.co/spaces/Roxanne-WANG/DeepSeeker_RAG

## Models

- **Embedding Model:**  
  [SentenceTransformer: all‑MiniLM‑L6‑v2](https://www.sbert.net/) is used for generating high-quality semantic embeddings.

- **Language Model:**  
  [GPT‑3.5 Turbo](https://openai.com/api/) via the OpenAI ChatCompletion API is used to generate contextually enriched responses.

## Performance Evaluation

The project includes an evaluation module that:
- Runs a set of representative queries through the full pipeline.
- Measures end-to-end response time for each query.
- Provides both quantitative metrics (latency) and qualitative assessments (relevance and coherence of responses).

Preliminary testing shows that the average response time is within acceptable limits and the generated responses are contextually accurate and enriched by domain-specific knowledge.

## Setup & Installation

### Installation

1. **Clone the Repository:**

```bash
git clone https://github.com/RoxanneWAAANG/DeepSeeker.git
cd DeepSeeker_RAG
```

2. Create a Virtual Environment:

```bash
python -m venv venv
source venv/bin/activate  # On Windows, use: venv\Scripts\activate
```

3. Install Dependencies:

```bash
pip install -r requirements.txt
```

4. Environment Variables: Create a .env file in the root directory with your OpenAI API key:

```ini
OPENAI_API_KEY=your_openai_api_key_here
TOKENIZERS_PARALLELISM=false
```

### Running the Application
1. Local Testing
- Run the Streamlit Frontend:

```bash
python -m streamlit run app.py
```

- Evaluation:

```bash
python evaluation.py
```

### Repository Structure
- `extract_pdf.py` – Extracts text from PDF files.
- `preprocess.py` – Cleans and splits text into chunks.
- `embeddings.py` – Generates normalized embeddings for text chunks.
- `db_setup.py` – Sets up the SQLite database for storing embeddings.
- `retrieval.py` – Implements semantic retrieval from SQLite.
- `rag_pipeline.py` – Combines retrieval and LLM generation into the RAG pipeline.
- `evaluation.py` – Contains code for performance evaluation.
- `app.py` – Streamlit frontend for user interaction.
- `requirements.txt` – Lists all project dependencies.